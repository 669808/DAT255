{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 14.02.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Unlike humans, who can understand and process natural language directly, our computational models in NLP require text to be presented in a numerical, encoded format. Here's a high-level overview of the steps involved in a process of converting text into a format that can be used as input to a model:\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "First, we perform a process called tokenization, which breaks down text into smaller units called tokens. \n",
    "\n",
    "Tokenization is not a one-size-fits-all process, and there are multiple tokenization strategies, each with its strengths and weaknesses depending on the task. We'll look at some in this notebook. \n",
    "\n",
    "**Numericalization**\n",
    "\n",
    "Once we have tokens, we convert them to numbers through numericalization. This is crucial because computers are designed to work with numbers, not words. \n",
    "\n",
    "**Batching**\n",
    "\n",
    "Next, we group these numbers representing text into batches. Batching helps the computer handle lots of text simultaneously, making everything more efficient. The strategy for grouping text into batches can vary, affecting how the model learns and performs. We'll examine an effective batching approach for a language model.\n",
    "\n",
    "**Embeddings**\n",
    "\n",
    "Finally, we use embeddings to represent each token as a vector. Embeddings are used to capture more than just the identity of a token; they encode semantic relationships and meanings. This representation is critical for models to understand and predict based on the text. \n",
    "\n",
    "\n",
    "Taken together, these steps convert text data into batches of vectors that can be used as input to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The notebook is partly based on:**\n",
    "\n",
    "* fastbook, Chapter 10: [NLP Deep Dive: RNNs](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb), 2020\n",
    "* fastai course, Lesson 4: [Natural Language (NLP)](https://course.fast.ai/Lessons/lesson4.html), 2023\n",
    "* Sebastian Raschka, [Build a Large Language Model (From Skratch)](https://github.com/rasbt/LLMs-from-scratch), 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.all import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the works of William Shakespeare as our text data, as prepared by Andrej Karpathy here: https://cs.stanford.edu/people/karpathy/char-rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_shakespeare = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import urllib.request\n",
    "\n",
    "def print_text(url, nb_lines=10, wrap_width=70):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            for i in range(nb_lines):\n",
    "                line = f.readline().decode('utf-8').strip()\n",
    "                if line:\n",
    "                    print('\\n'.join(textwrap.wrap(line, wrap_width)))\n",
    "    except urllib.error.URLError:\n",
    "        print(f\"Error opening URL {url}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "All:\n",
      "Speak, speak.\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "All:\n",
      "Resolved. resolved.\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "All:\n",
      "We know't, we know't.\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "print_text(text_shakespeare, nb_lines=20, wrap_width=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple strategy is to tokenize the text into characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(test_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign numbers to the tokens, i.e., **numericalization**, we can simply encode each character with a unique integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {t: i for i, t in enumerate(sorted(set(tokens)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " ',': 2,\n",
       " '-': 3,\n",
       " '.': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '?': 8,\n",
       " 'A': 9,\n",
       " 'I': 10,\n",
       " 'T': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'h': 17,\n",
       " 'i': 18,\n",
       " 'l': 19,\n",
       " 'm': 20,\n",
       " 'n': 21,\n",
       " 'o': 22,\n",
       " 'p': 23,\n",
       " 'r': 24,\n",
       " 's': 25,\n",
       " 't': 26,\n",
       " 'u': 27}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go back from numericalized tokens to text, we can use the following mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token = {i: t for t, i in token2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '!',\n",
       " 2: ',',\n",
       " 3: '-',\n",
       " 4: '.',\n",
       " 5: '1',\n",
       " 6: '2',\n",
       " 7: '3',\n",
       " 8: '?',\n",
       " 9: 'A',\n",
       " 10: 'I',\n",
       " 11: 'T',\n",
       " 12: 'a',\n",
       " 13: 'b',\n",
       " 14: 'c',\n",
       " 15: 'd',\n",
       " 16: 'e',\n",
       " 17: 'h',\n",
       " 18: 'i',\n",
       " 19: 'l',\n",
       " 20: 'm',\n",
       " 21: 'n',\n",
       " 22: 'o',\n",
       " 23: 'p',\n",
       " 24: 'r',\n",
       " 25: 's',\n",
       " 26: 't',\n",
       " 27: 'u'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go back and forth between text and numericalized tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2idxs = np.array([token2idx[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 17, 18, 25,  0, 18, 25,  0, 12,  0, 26, 16, 25, 26,  0, 25, 16,\n",
       "       21, 26, 16, 21, 14, 16,  4,  0, 10, 26,  0, 18, 21, 14, 19, 27, 15,\n",
       "       16, 25,  0,  3,  3,  0, 25, 22, 20, 16,  0, 23, 27, 21, 14, 26, 27,\n",
       "       12, 26, 18, 22, 21,  1,  8,  0,  9, 21, 15,  0, 25, 22, 20, 16,  0,\n",
       "       21, 27, 20, 13, 16, 24, 25,  0,  5,  2,  0,  6,  2,  0,  7,  4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n"
     ]
    }
   ],
   "source": [
    "print(''.join([idx2token[i] for i in tokens2idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenization strategy ignores the structure in the text and is not very useful for most language tasks. Rather, we want to preserve some of the structure in the text when tokenizing it. Let's look at some other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize by matching string patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenization(text, pattern, include_whitespace=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on the provided pattern.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to tokenize.\n",
    "    pattern (str): The pattern to use for tokenization.\n",
    "    include_whitespace (bool): If True, includes whitespace as a tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of tokens.\n",
    "    \"\"\"\n",
    "    result = re.split(pattern, text)\n",
    "    if not include_whitespace:\n",
    "        result = [item.strip() for item in result if item.strip()]\n",
    "    if verbose:\n",
    "        print(\"Whitespace included:\", include_whitespace)\n",
    "        print(\"Input:\", text)\n",
    "        print(\"Length of input:\", len(text))\n",
    "        print(\"Result:\", result)\n",
    "        print(\"Number of tokens:\", len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization by splitting on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence.', 'It', 'includes', '--', 'some', 'punctuation!?', 'And', 'some', 'numbers', '1,', '2,', '3.']\n",
      "Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'(\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: True\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', ' ', 'is', ' ', 'a', ' ', 'test', ' ', 'sentence.', ' ', 'It', ' ', 'includes', ' ', '--', ' ', 'some', ' ', 'punctuation!?', ' ', 'And', ' ', 'some', ' ', 'numbers', ' ', '1,', ' ', '2,', ' ', '3.']\n",
      "Number of tokens: 31\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'(\\s)', include_whitespace=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would never use this in practice, as it's very inefficient and uses no features of language except that words tend to, in many languages, be separated by spaces. Among other things, we lose punctuation and the fact that some words are contractions of multiple words (for example \"here's\", \"isn't\", and \"don't\"). By specifying a set of rules, we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spacy.io/images/tokenization.svg\"> <br> <small>Illustration from spaCy's documentation: https://spacy.io/usage/linguistic-features#tokenization</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split on whitespace and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on commas and periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence', '.', 'It', 'includes', '--', 'some', 'punctuation!?', 'And', 'some', 'numbers', '1', ',', '2', ',', '3', '.']\n",
      "Number of tokens: 20\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'([,.]|\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence', '.', 'It', 'includes', '--', 'some', 'punctuation', '!', '?', 'And', 'some', 'numbers', '1', ',', '2', ',', '3', '.']\n",
      "Number of tokens: 22\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'([,.?_!\"()\\']|--|\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Experiment with different pattern values in the `basic_tokenizer` function. Note the effect on the number of tokens and the tokens themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tokenization to the entire text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'([,.?_!\"():&\\']|--|\\s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with urllib.request.urlopen(text_shakespeare) as response:\n",
    "        data = response.read().decode('utf-8')\n",
    "\n",
    "    preprocessed = basic_tokenization(data, pattern, include_whitespace=False)\n",
    "    preprocessed = [item.lower() for item in preprocessed]\n",
    "    # Get the frequency of each token\n",
    "    token_counts = pd.Series(preprocessed).value_counts()\n",
    "    # Get the number of unique tokens\n",
    "    nb_tokens = len(token_counts)\n",
    "\n",
    "except urllib.error.URLError:\n",
    "        print(f\"Error opening URL {text_shakespeare}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054375,\n",
       " ['first',\n",
       "  'citizen',\n",
       "  ':',\n",
       "  'before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.',\n",
       "  'all',\n",
       "  ':',\n",
       "  'speak',\n",
       "  ',',\n",
       "  'speak',\n",
       "  '.',\n",
       "  'first'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed), preprocessed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most common tokens in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",      79977\n",
       ":      44576\n",
       ".      33850\n",
       "the    26221\n",
       "'      24028\n",
       "and    23528\n",
       "i      21808\n",
       "to     18163\n",
       "of     16341\n",
       "you    13551\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29021"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of tokens\n",
    "nb_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to map a string to a set of tokens, we need to have a fixed set of tokens to map to. Moreover, if we want to decode a set of tokens to a string, we need to have a fixed set of tokens to map from. This is the motivation for building a **vocabulary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider our test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we recall, this can be tokenized by splitting on whitespace and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'([,.?_!\"():&\\']|--|\\s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'It',\n",
       " 'includes',\n",
       " '--',\n",
       " 'some',\n",
       " 'punctuation',\n",
       " '!',\n",
       " '?',\n",
       " 'And',\n",
       " 'some',\n",
       " 'numbers',\n",
       " '1',\n",
       " ',',\n",
       " '2',\n",
       " ',',\n",
       " '3',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = basic_tokenization(test_text, pattern, include_whitespace=False, verbose=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can construct a vocabulary. Here's a first, simple approach: sort the tokens alphabetically and assign an index to each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(tokens)))\n",
    "vocab_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '?',\n",
       " 'And',\n",
       " 'It',\n",
       " 'This',\n",
       " 'a',\n",
       " 'includes',\n",
       " 'is',\n",
       " 'numbers',\n",
       " 'punctuation',\n",
       " 'sentence',\n",
       " 'some',\n",
       " 'test']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can map the unique tokens to unique token indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " ',': 1,\n",
       " '--': 2,\n",
       " '.': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '?': 7,\n",
       " 'And': 8,\n",
       " 'It': 9,\n",
       " 'This': 10,\n",
       " 'a': 11,\n",
       " 'includes': 12,\n",
       " 'is': 13,\n",
       " 'numbers': 14,\n",
       " 'punctuation': 15,\n",
       " 'sentence': 16,\n",
       " 'some': 17,\n",
       " 'test': 18}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/HVL-ML/DAT255/blob/main/nbs/assets/building_a_vocabulary_from_corpus.png?raw=true\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a vocabulary from the entire text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the entire text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29021"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that having a large vocab size means that we will end up with a large number of parameters in our model. For example, if you have a vocab size of 100,000, and your first layer in the network compresses this to 1024 dimensional word embeddings, then the first layer will have 100.000 x 1000 = 100 million parameters. Just for the first layer! This is why we often use subword tokenization, which we'll look at later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('&', 1)\n",
      "(\"'\", 2)\n",
      "(',', 3)\n",
      "('-', 4)\n",
      "('--', 5)\n",
      "('-but', 6)\n",
      "('-groves', 7)\n",
      "('-guts', 8)\n",
      "('.', 9)\n",
      "('3', 10)\n",
      "(':', 11)\n",
      "(';', 12)\n",
      "('?', 13)\n",
      "('[and', 14)\n",
      "('[gower]', 15)\n",
      "('[prospero]', 16)\n",
      "(']', 17)\n",
      "('a', 18)\n",
      "('a-bed', 19)\n",
      "('a-bed;', 20)\n",
      "('a-birding', 21)\n",
      "('a-birding;', 22)\n",
      "('a-bleeding', 23)\n",
      "('a-bleeding;', 24)\n",
      "('a-breeding', 25)\n",
      "('a-brewing', 26)\n",
      "('a-cold', 27)\n",
      "('a-cursing', 28)\n",
      "('a-day', 29)\n",
      "('a-doing', 30)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for item in itertools.islice(vocab.items(), 31):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a class to wrap our encoding and decoding functions. It needs to store the vocab and provide a method to encode a string and decode a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTokenizerV1:\n",
    "    def __init__(self, vocab, pattern):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with a vocabulary.\n",
    "        \n",
    "        Parameters:\n",
    "        vocab (dict): A dictionary mapping strings to integers.\n",
    "        pattern (str): The pattern to use for tokenization.\n",
    "        \"\"\"\n",
    "        self.pattern = pattern\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of integers.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to encode.\n",
    "        \n",
    "        Returns:\n",
    "        list: The encoded text.\n",
    "        \"\"\"\n",
    "        preprocessed = basic_tokenization(text, self.pattern, include_whitespace=False)\n",
    "        preprocessed = [item.lower() for item in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]  \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of integers into a text string.\n",
    "        \n",
    "        Parameters:\n",
    "        ids (list): The list of integers to decode.\n",
    "        \n",
    "        Returns:\n",
    "        str: The decoded text.\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(self.pattern, r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it using the pattern and vocab from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'([,.?_!\"():&\\\\\\']|--|\\\\s)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = OurTokenizerV1(vocab, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9703, 3, 28983, 14013, 3553, 15403, 13517, 4288, 8308, 25668, 25232, 18374, 9]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"First, you know Caius Marcius is chief enemy to the people.\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first , you know caius marcius is chief enemy to the people .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use a token that is not in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 26\u001b[0m, in \u001b[0;36mOurTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     24\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m basic_tokenization(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern, include_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'vocabulary'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(\"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to deal with this somehow, as we can't guarantee that the tokenized text will only contain tokens from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with unknown tokens, we can add a special token to the vocabulary: the `<|unk|>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.extend([\"<|unk|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zodiacs', 29017)\n",
      "('zone', 29018)\n",
      "('zounds', 29019)\n",
      "('zwaggered', 29020)\n",
      "('<|unk|>', 29021)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new token is added and has the token ID of 29021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small update to our tokenizer class can handle unknown tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTokenizerV2:\n",
    "    def __init__(self, vocab, pattern):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with a vocabulary.\n",
    "        \n",
    "        Parameters:\n",
    "        vocab (dict): A dictionary mapping strings to integers.\n",
    "        pattern (str): The pattern to use for tokenization.\n",
    "        \"\"\"\n",
    "        self.pattern = pattern\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of integers.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to encode.\n",
    "        \n",
    "        Returns:\n",
    "        list: The encoded text.\n",
    "        \"\"\"\n",
    "        preprocessed = basic_tokenization(text, self.pattern, include_whitespace=False)\n",
    "        preprocessed = [item.lower() for item in preprocessed]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]  \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of integers into a text string.\n",
    "        \n",
    "        Parameters:\n",
    "        ids (list): The list of integers to decode.\n",
    "        \n",
    "        Returns:\n",
    "        str: The decoded text.\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(self.pattern, r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = OurTokenizerV2(vocab, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25342,\n",
       " 22245,\n",
       " 5330,\n",
       " 28728,\n",
       " 25224,\n",
       " 1209,\n",
       " 17079,\n",
       " 12950,\n",
       " 25232,\n",
       " 29021,\n",
       " 9,\n",
       " 10044,\n",
       " 8859,\n",
       " 3,\n",
       " 29021]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = tokenizer2.encode(\"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\")\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this sentence contains words that are not in the <|unk|> . for example , <|unk|>'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword tokenization and byte-pair encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenization is a method that finds a middle ground between breaking down text into single characters and keeping them as whole words. It works by splitting words into smaller pieces. This way, common words stay as they are, but rare words get broken down into parts that we see more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern subword tokenizers tend to be _trained_ on the text corpus you're interested in (or pre-trained on a large corpus that you want to train a model that you can use for fine-tuning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair encoding: an example of training an encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with a particular text corpus the above rules-based tokenizers can often be both wasteful (with superfluous tokens for words that doen't appear in your corpus) and inefficient (for example, lacking tokens that can represent important and often-used words in your particular corpus). \n",
    "\n",
    "Tokenizers based on _training_, i.e. identification of important words or word pieces, can therefore be useful, and this is thus part of most modern tokenizers. An example is the **byte pair encoding** used by, among others, GPT models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding (BPE) was first introduced for data compression by finding and combining common pairs of letters or bytes. \n",
    "\n",
    "> It was introduced by Philip Gage in 1994 for data compression (_\"a simple general-purpose data compression algorithm\"_), based on identifying common byte pairs. Here's a copy of the original article http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM. See also Wikipedia for a simple example of BPE used for data compression: https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
    "\n",
    "Currently, it's commonly used in language processing to figure out which pieces of words to combine to make our vocabulary better suited to the text we're working with. \n",
    "\n",
    "The procedure is roughly the following: \n",
    "\n",
    "```\n",
    "1. Add identifiers marking the end of each word\n",
    "2. Calculate the word frequencies in the text corpus\n",
    "3. Split the words into characters and calculate the character frequencies\n",
    "4. From character tokens, count the frequency of consecutive byte pairs and merge the most frequent byte pair\n",
    "5. Continue until a manually defined iteration limit is reached, or the token limit is reached. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is straightforward and focuses on what's seen most often. It makes our vocabulary more relevant to the text, especially when dealing with unusual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a greedy algorithm. Non-greedy variants exist and other tweaks to BPE are in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pros and cons: Using BPE makes our vocabulary more flexible and efficient. It's especially good at handling rare words without needing a huge list of every possible word. However, because it always looks for the most common pairs, it might not always create the best possible combination for every situation. Plus, figuring out when to stop combining letters can be tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than implementing BPE from scratch, we can use the `tokenizers` library to grab a trained BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212,\n",
       " 6827,\n",
       " 4909,\n",
       " 2456,\n",
       " 326,\n",
       " 389,\n",
       " 407,\n",
       " 287,\n",
       " 262,\n",
       " 25818,\n",
       " 13,\n",
       " 1114,\n",
       " 1672,\n",
       " 11,\n",
       " 1885,\n",
       " 29207,\n",
       " 44390,\n",
       " 3699,\n",
       " 1042]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = tokenizer.encode(test_text)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 19)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text.split()), len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[415, 29207, 44390, 3699, 1042]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant\n",
      "idis\n",
      "establishment\n",
      "arian\n",
      "ism\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(\"antidisestablishmentarianism\"):\n",
    "    print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TBA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TBA_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
