{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 22.02.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unlike humans, who can understand and process natural language directly, our computational models in NLP require text to be presented in a numerical, encoded format. Here's a high-level overview of the steps involved in a process of converting text into a format that can be used as input to a model:\n",
    "\n",
    "**Tokenization**\n",
    "\n",
    "First, we perform a process called tokenization, which breaks down text into smaller units called tokens. \n",
    "\n",
    "Tokenization is not a one-size-fits-all process, and there are multiple tokenization strategies, each with its strengths and weaknesses depending on the task. We'll look at some in this notebook. \n",
    "\n",
    "**Numericalization**\n",
    "\n",
    "Once we have tokens, we convert them to numbers through numericalization. This is crucial because computers are designed to work with numbers, not words. \n",
    "\n",
    "**Batching**\n",
    "\n",
    "Next, we group these numbers representing text into batches. Batching helps the computer handle lots of text simultaneously, making everything more efficient. The strategy for grouping text into batches can vary, affecting how the model learns and performs. We'll examine an effective batching approach for a language model.\n",
    "\n",
    "**Embeddings**\n",
    "\n",
    "Finally, we use embeddings to represent each token as a vector. Embeddings are used to capture more than just the identity of a token; they encode semantic relationships and meanings. This representation is critical for models to understand and predict based on the text. \n",
    "\n",
    "\n",
    "Taken together, these steps convert text data into batches of vectors that can be used as input to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The notebook is partly based on:**\n",
    "\n",
    "* fastbook, Chapter 10: [NLP Deep Dive: RNNs](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb), 2020\n",
    "* fastai course, Lesson 4: [Natural Language (NLP)](https://course.fast.ai/Lessons/lesson4.html), 2023\n",
    "* Sebastian Raschka, [Build a Large Language Model (From Skratch)](https://github.com/rasbt/LLMs-from-scratch), 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or Kaggle, which makes some difference for the code below.\n",
    "try:\n",
    "    import google.colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    %pip install -Uqq fastcore tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcore.all import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the works of William Shakespeare as our text data, as prepared by Andrej Karpathy here: https://cs.stanford.edu/people/karpathy/char-rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_shakespeare = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import urllib.request\n",
    "\n",
    "def print_text(url, nb_lines=10, wrap_width=70):\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            for i in range(nb_lines):\n",
    "                line = f.readline().decode('utf-8').strip()\n",
    "                if line:\n",
    "                    print('\\n'.join(textwrap.wrap(line, wrap_width)))\n",
    "    except urllib.error.URLError:\n",
    "        print(f\"Error opening URL {url}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "All:\n",
      "Speak, speak.\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "All:\n",
      "Resolved. resolved.\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "All:\n",
      "We know't, we know't.\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "print_text(text_shakespeare, nb_lines=20, wrap_width=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple strategy is to tokenize the text into characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = list(test_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign numbers to the tokens, i.e., **numericalization**, we can simply encode each character with a unique integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx = {t: i for i, t in enumerate(sorted(set(tokens)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " ',': 2,\n",
       " '-': 3,\n",
       " '.': 4,\n",
       " '1': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '?': 8,\n",
       " 'A': 9,\n",
       " 'I': 10,\n",
       " 'T': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'h': 17,\n",
       " 'i': 18,\n",
       " 'l': 19,\n",
       " 'm': 20,\n",
       " 'n': 21,\n",
       " 'o': 22,\n",
       " 'p': 23,\n",
       " 'r': 24,\n",
       " 's': 25,\n",
       " 't': 26,\n",
       " 'u': 27}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go back from numericalized tokens to text, we can use the following mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token = {i: t for t, i in token2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '!',\n",
       " 2: ',',\n",
       " 3: '-',\n",
       " 4: '.',\n",
       " 5: '1',\n",
       " 6: '2',\n",
       " 7: '3',\n",
       " 8: '?',\n",
       " 9: 'A',\n",
       " 10: 'I',\n",
       " 11: 'T',\n",
       " 12: 'a',\n",
       " 13: 'b',\n",
       " 14: 'c',\n",
       " 15: 'd',\n",
       " 16: 'e',\n",
       " 17: 'h',\n",
       " 18: 'i',\n",
       " 19: 'l',\n",
       " 20: 'm',\n",
       " 21: 'n',\n",
       " 22: 'o',\n",
       " 23: 'p',\n",
       " 24: 'r',\n",
       " 25: 's',\n",
       " 26: 't',\n",
       " 27: 'u'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go back and forth between text and numericalized tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2idxs = np.array([token2idx[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 17, 18, 25,  0, 18, 25,  0, 12,  0, 26, 16, 25, 26,  0, 25, 16,\n",
       "       21, 26, 16, 21, 14, 16,  4,  0, 10, 26,  0, 18, 21, 14, 19, 27, 15,\n",
       "       16, 25,  0,  3,  3,  0, 25, 22, 20, 16,  0, 23, 27, 21, 14, 26, 27,\n",
       "       12, 26, 18, 22, 21,  1,  8,  0,  9, 21, 15,  0, 25, 22, 20, 16,  0,\n",
       "       21, 27, 20, 13, 16, 24, 25,  0,  5,  2,  0,  6,  2,  0,  7,  4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n"
     ]
    }
   ],
   "source": [
    "print(''.join([idx2token[i] for i in tokens2idxs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenization strategy ignores the structure in the text and is not very useful for most language tasks. Rather, we want to preserve some of the structure in the text when tokenizing it. Let's look at some other strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize by matching string patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenization(text, pattern, include_whitespace=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text based on the provided pattern.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to tokenize.\n",
    "    pattern (str): The pattern to use for tokenization.\n",
    "    include_whitespace (bool): If True, includes whitespace as a tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of tokens.\n",
    "    \"\"\"\n",
    "    result = re.split(pattern, text)\n",
    "    if not include_whitespace:\n",
    "        result = [item.strip() for item in result if item.strip()]\n",
    "    if verbose:\n",
    "        print(\"Whitespace included:\", include_whitespace)\n",
    "        print(\"Input:\", text)\n",
    "        print(\"Length of input:\", len(text))\n",
    "        print(\"Result:\", result)\n",
    "        print(\"Number of tokens:\", len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization by splitting on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence.', 'It', 'includes', '--', 'some', 'punctuation!?', 'And', 'some', 'numbers', '1,', '2,', '3.']\n",
      "Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'(\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: True\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', ' ', 'is', ' ', 'a', ' ', 'test', ' ', 'sentence.', ' ', 'It', ' ', 'includes', ' ', '--', ' ', 'some', ' ', 'punctuation!?', ' ', 'And', ' ', 'some', ' ', 'numbers', ' ', '1,', ' ', '2,', ' ', '3.']\n",
      "Number of tokens: 31\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'(\\s)', include_whitespace=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One would never use this in practice, as it's very inefficient and uses no features of language except that words tend to, in many languages, be separated by spaces. Among other things, we lose punctuation and the fact that some words are contractions of multiple words (for example \"here's\", \"isn't\", and \"don't\"). By specifying a set of rules, we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spacy.io/images/tokenization.svg\"> <br> <small>Illustration from spaCy's documentation: https://spacy.io/usage/linguistic-features#tokenization</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split on whitespace and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on commas and periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence', '.', 'It', 'includes', '--', 'some', 'punctuation!?', 'And', 'some', 'numbers', '1', ',', '2', ',', '3', '.']\n",
      "Number of tokens: 20\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'([,.]|\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add additional punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace included: False\n",
      "Input: This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.\n",
      "Length of input: 84\n",
      "Result: ['This', 'is', 'a', 'test', 'sentence', '.', 'It', 'includes', '--', 'some', 'punctuation', '!', '?', 'And', 'some', 'numbers', '1', ',', '2', ',', '3', '.']\n",
      "Number of tokens: 22\n"
     ]
    }
   ],
   "source": [
    "_ = basic_tokenization(test_text, r'([,.?_!\"()\\']|--|\\s)', include_whitespace=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Experiment with different pattern values in the `basic_tokenizer` function. Note the effect on the number of tokens and the tokens themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tokenization to the entire text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'([,.?_!\"():&\\']|--|\\s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with urllib.request.urlopen(text_shakespeare) as response:\n",
    "        data = response.read().decode('utf-8')\n",
    "\n",
    "    preprocessed = basic_tokenization(data, pattern, include_whitespace=False)\n",
    "    preprocessed = [item.lower() for item in preprocessed]\n",
    "    # Get the frequency of each token\n",
    "    token_counts = pd.Series(preprocessed).value_counts()\n",
    "    # Get the number of unique tokens\n",
    "    nb_tokens = len(token_counts)\n",
    "\n",
    "except urllib.error.URLError:\n",
    "        print(f\"Error opening URL {text_shakespeare}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1054375,\n",
       " ['first',\n",
       "  'citizen',\n",
       "  ':',\n",
       "  'before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.',\n",
       "  'all',\n",
       "  ':',\n",
       "  'speak',\n",
       "  ',',\n",
       "  'speak',\n",
       "  '.',\n",
       "  'first'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed), preprocessed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the most common tokens in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",      79977\n",
       ":      44576\n",
       ".      33850\n",
       "the    26221\n",
       "'      24028\n",
       "and    23528\n",
       "i      21808\n",
       "to     18163\n",
       "of     16341\n",
       "you    13551\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29021"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of tokens\n",
    "nb_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to map a string to a set of tokens, we need to have a fixed set of tokens to map to. Moreover, if we want to decode a set of tokens to a string, we need to have a fixed set of tokens to map from. This is the motivation for building a **vocabulary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider our test text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence. It includes -- some punctuation!? And some numbers 1, 2, 3.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we recall, this can be tokenized by splitting on whitespace and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'([,.?_!\"():&\\']|--|\\s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'It',\n",
       " 'includes',\n",
       " '--',\n",
       " 'some',\n",
       " 'punctuation',\n",
       " '!',\n",
       " '?',\n",
       " 'And',\n",
       " 'some',\n",
       " 'numbers',\n",
       " '1',\n",
       " ',',\n",
       " '2',\n",
       " ',',\n",
       " '3',\n",
       " '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = basic_tokenization(test_text, pattern, include_whitespace=False, verbose=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can construct a vocabulary. Here's a first, simple approach: sort the tokens alphabetically and assign an index to each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(tokens)))\n",
    "vocab_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '?',\n",
       " 'And',\n",
       " 'It',\n",
       " 'This',\n",
       " 'a',\n",
       " 'includes',\n",
       " 'is',\n",
       " 'numbers',\n",
       " 'punctuation',\n",
       " 'sentence',\n",
       " 'some',\n",
       " 'test']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can map the unique tokens to unique token indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " ',': 1,\n",
       " '--': 2,\n",
       " '.': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '?': 7,\n",
       " 'And': 8,\n",
       " 'It': 9,\n",
       " 'This': 10,\n",
       " 'a': 11,\n",
       " 'includes': 12,\n",
       " 'is': 13,\n",
       " 'numbers': 14,\n",
       " 'punctuation': 15,\n",
       " 'sentence': 16,\n",
       " 'some': 17,\n",
       " 'test': 18}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/HVL-ML/DAT255/blob/main/nbs/assets/building_a_vocabulary_from_corpus.png?raw=true\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a vocabulary from the entire text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the entire text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29021"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that having a large vocab size means that we will end up with a large number of parameters in our model. For example, if you have a vocab size of 100,000, and your first layer in the network compresses this to 1024 dimensional word embeddings, then the first layer will have 100.000 x 1000 = 100 million parameters. Just for the first layer! This is why we often use subword tokenization, which we'll look at later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('&', 1)\n",
      "(\"'\", 2)\n",
      "(',', 3)\n",
      "('-', 4)\n",
      "('--', 5)\n",
      "('-but', 6)\n",
      "('-groves', 7)\n",
      "('-guts', 8)\n",
      "('.', 9)\n",
      "('3', 10)\n",
      "(':', 11)\n",
      "(';', 12)\n",
      "('?', 13)\n",
      "('[and', 14)\n",
      "('[gower]', 15)\n",
      "('[prospero]', 16)\n",
      "(']', 17)\n",
      "('a', 18)\n",
      "('a-bed', 19)\n",
      "('a-bed;', 20)\n",
      "('a-birding', 21)\n",
      "('a-birding;', 22)\n",
      "('a-bleeding', 23)\n",
      "('a-bleeding;', 24)\n",
      "('a-breeding', 25)\n",
      "('a-brewing', 26)\n",
      "('a-cold', 27)\n",
      "('a-cursing', 28)\n",
      "('a-day', 29)\n",
      "('a-doing', 30)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for item in itertools.islice(vocab.items(), 31):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a class to wrap our encoding and decoding functions. It needs to store the vocab and provide a method to encode a string and decode a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTokenizerV1:\n",
    "    def __init__(self, vocab, pattern):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with a vocabulary.\n",
    "        \n",
    "        Parameters:\n",
    "        vocab (dict): A dictionary mapping strings to integers.\n",
    "        pattern (str): The pattern to use for tokenization.\n",
    "        \"\"\"\n",
    "        self.pattern = pattern\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of integers.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to encode.\n",
    "        \n",
    "        Returns:\n",
    "        list: The encoded text.\n",
    "        \"\"\"\n",
    "        preprocessed = basic_tokenization(text, self.pattern, include_whitespace=False)\n",
    "        preprocessed = [item.lower() for item in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]  \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of integers into a text string.\n",
    "        \n",
    "        Parameters:\n",
    "        ids (list): The list of integers to decode.\n",
    "        \n",
    "        Returns:\n",
    "        str: The decoded text.\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(self.pattern, r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it using the pattern and vocab from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'([,.?_!\"():&\\\\\\']|--|\\\\s)'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = OurTokenizerV1(vocab, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9703, 3, 28983, 14013, 3553, 15403, 13517, 4288, 8308, 25668, 25232, 18374, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"First, you know Caius Marcius is chief enemy to the people.\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first , you know caius marcius is chief enemy to the people .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use a token that is not in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m, in \u001b[0;36mOurTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m basic_tokenization(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern, include_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m basic_tokenization(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern, include_whitespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'vocabulary'"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(\"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to deal with this somehow, as we can't guarantee that the tokenized text will only contain tokens from the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with unknown tokens, we can add a special token to the vocabulary: the `<|unk|>` token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.extend([\"<|unk|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('zodiacs', 29017)\n",
      "('zone', 29018)\n",
      "('zounds', 29019)\n",
      "('zwaggered', 29020)\n",
      "('<|unk|>', 29021)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new token is added and has the token ID of 29021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small update to our tokenizer class can handle unknown tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTokenizerV2:\n",
    "    def __init__(self, vocab, pattern):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer with a vocabulary.\n",
    "        \n",
    "        Parameters:\n",
    "        vocab (dict): A dictionary mapping strings to integers.\n",
    "        pattern (str): The pattern to use for tokenization.\n",
    "        \"\"\"\n",
    "        self.pattern = pattern\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode a text string into a list of integers.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to encode.\n",
    "        \n",
    "        Returns:\n",
    "        list: The encoded text.\n",
    "        \"\"\"\n",
    "        preprocessed = basic_tokenization(text, self.pattern, include_whitespace=False)\n",
    "        preprocessed = [item.lower() for item in preprocessed]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]  \n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode a list of integers into a text string.\n",
    "        \n",
    "        Parameters:\n",
    "        ids (list): The list of integers to decode.\n",
    "        \n",
    "        Returns:\n",
    "        str: The decoded text.\n",
    "        \"\"\"\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "\n",
    "        text = re.sub(self.pattern, r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = OurTokenizerV2(vocab, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25342,\n",
       " 22245,\n",
       " 5330,\n",
       " 28728,\n",
       " 25224,\n",
       " 1209,\n",
       " 17079,\n",
       " 12950,\n",
       " 25232,\n",
       " 29021,\n",
       " 9,\n",
       " 10044,\n",
       " 8859,\n",
       " 3,\n",
       " 29021]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = tokenizer2.encode(\"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\")\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this sentence contains words that are not in the <|unk|> . for example , <|unk|>'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword tokenization and byte-pair encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenization is a method that finds a middle ground between breaking down text into single characters and keeping them as whole words. It works by splitting words into smaller pieces. This way, common words stay as they are, but rare words get broken down into parts that we see more often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern subword tokenizers tend to be _trained_ on the text corpus you're interested in (or pre-trained on a large corpus that you want to train a model that you can use for fine-tuning). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair encoding: an example of training an encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When faced with a particular text corpus the above rules-based tokenizers can often be both wasteful (with superfluous tokens for words that doen't appear in your corpus) and inefficient (for example, lacking tokens that can represent important and often-used words in your particular corpus). \n",
    "\n",
    "Tokenizers based on _training_, i.e. identification of important words or word pieces, can therefore be useful, and this is thus part of most modern tokenizers. An example is the **byte pair encoding** used by, among others, GPT models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding (BPE) was first introduced for data compression by finding and combining common pairs of letters or bytes. \n",
    "\n",
    "> It was introduced by Philip Gage in 1994 for data compression (_\"a simple general-purpose data compression algorithm\"_), based on identifying common byte pairs. Here's a copy of the original article http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM. See also Wikipedia for a simple example of BPE used for data compression: https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
    "\n",
    "Currently, it's commonly used in language processing to figure out which pieces of words to combine to make our vocabulary better suited to the text we're working with. \n",
    "\n",
    "The procedure is roughly the following: \n",
    "\n",
    "```\n",
    "1. Add identifiers marking the end of each word\n",
    "2. Calculate the word frequencies in the text corpus\n",
    "3. Split the words into characters and calculate the character frequencies\n",
    "4. From character tokens, count the frequency of consecutive byte pairs and merge the most frequent byte pair\n",
    "5. Continue until a manually defined iteration limit is reached, or the token limit is reached. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is straightforward and focuses on what's seen most often. It makes our vocabulary more relevant to the text, especially when dealing with unusual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is a greedy algorithm. Non-greedy variants exist and other tweaks to BPE are in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pros and cons: Using BPE makes our vocabulary more flexible and efficient. It's especially good at handling rare words without needing a huge list of every possible word. However, because it always looks for the most common pairs, it might not always create the best possible combination for every situation. Plus, figuring out when to stop combining letters can be tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than implementing BPE from scratch, we can use `tiktoken` to grab a trained BPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uqq tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212,\n",
       " 6827,\n",
       " 4909,\n",
       " 2456,\n",
       " 326,\n",
       " 389,\n",
       " 407,\n",
       " 287,\n",
       " 262,\n",
       " 25818,\n",
       " 13,\n",
       " 1114,\n",
       " 1672,\n",
       " 11,\n",
       " 1885,\n",
       " 29207,\n",
       " 44390,\n",
       " 3699,\n",
       " 1042]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs = tokenizer.encode(test_text)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 19)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text.split()), len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This sentence contains words that are not in the vocabulary. For example, antidisestablishmentarianism'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[415, 29207, 44390, 3699, 1042]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"antidisestablishmentarianism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ant\n",
      "idis\n",
      "establishment\n",
      "arian\n",
      "ism\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(\"antidisestablishmentarianism\"):\n",
    "    print(tokenizer.decode([i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_TBA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've learned how to break down text into tokens (words, subwords, etc.) and represent them as simple numerical indices. While that's a necessary first step, there are two problems: \n",
    "\n",
    "(i) These numerical representations don't capture the meaning or relationships between words;<br>\n",
    "(ii) our deep neural network models require continuous vector representations as inputs, as they are trained using backpropagation and gradient descent.\n",
    "\n",
    "This is where embeddings come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Vector representations:** Embeddings represent words (or tokens) as dense vectors of numbers, aiming to place words with similar contexts closer together.\n",
    "\n",
    "**Semantic similarity:** Instead of just an arbitrary number, an embedding vector tries to capture a word's meaning and its relationships with other words. Semantically similar words (e.g., 'cat' and 'dog') would ideally have embeddings closer together in the vector space.\n",
    "\n",
    "> Note that embeddings aren't restricted to words or word-based tokens. They can also represent entire sentences, paragraphs, or even code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pre-trained embeddings are embeddings trained on a large corpus of text. They are often trained using unsupervised learning methods like word2vec, GloVe, or fastText. These embeddings are then used as a starting point for training a model on a specific task, such as sentiment analysis or named entity recognition. As the embeddings are trained on massive text datasets, they can contain rich semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uqq torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = vocab.GloVe(name='6B', dim=100) # Example: Load 100-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the embedding for a word\n",
    "word_embedding = glove.get_vecs_by_tokens('hello') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2669,  0.3963,  0.6169, -0.7745, -0.1039,  0.2670,  0.2788,  0.3099,\n",
       "         0.0055, -0.0853,  0.7360, -0.0984,  0.5479, -0.0303,  0.3348,  0.1409,\n",
       "        -0.0070,  0.3257,  0.2290,  0.4656, -0.1953,  0.3749, -0.7139, -0.5178,\n",
       "         0.7704,  1.0881, -0.6601, -0.1623,  0.9119,  0.2105,  0.0475,  1.0019,\n",
       "         1.1133,  0.7009, -0.0870,  0.4757,  0.1636, -0.4447,  0.4469, -0.9382,\n",
       "         0.0131,  0.0860, -0.6746,  0.4966, -0.0378, -0.1104, -0.2861,  0.0746,\n",
       "        -0.3153, -0.0938, -0.5707,  0.6686,  0.4531, -0.3415, -0.7166, -0.7527,\n",
       "         0.0752,  0.5790, -0.1191, -0.1138, -0.1003,  0.7134, -1.1574, -0.7403,\n",
       "         0.4045,  0.1802,  0.2145,  0.3764,  0.1124, -0.5364, -0.0251,  0.3189,\n",
       "        -0.2501, -0.6328, -0.0118,  1.3770,  0.8601,  0.2048, -0.3681, -0.6887,\n",
       "         0.5351, -0.4656,  0.2739,  0.4118, -0.8540, -0.0463,  0.1130, -0.2733,\n",
       "         0.1564, -0.2033,  0.5359,  0.5978,  0.6047,  0.1373,  0.4223, -0.6128,\n",
       "        -0.3849,  0.3584, -0.4846,  0.3073])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring distance and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between 'cat' and 'dog':\n",
      "tensor(2.6811)\n"
     ]
    }
   ],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "\n",
    "print(\"Euclidean distance between 'cat' and 'dog':\")\n",
    "print(torch.norm(y - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between 'cat' and 'continent':\n",
      "tensor(6.5621)\n"
     ]
    }
   ],
   "source": [
    "# Euclidean distance\n",
    "\n",
    "x = glove['cat']\n",
    "y = glove['continent']\n",
    "\n",
    "print(\"Euclidean distance between 'cat' and 'continent':\")\n",
    "print(torch.norm(y - x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'cat' and 'dog':\n",
      "tensor([0.8798])\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "\n",
    "x = glove['cat']\n",
    "y = glove['dog']\n",
    "\n",
    "print(\"Cosine similarity between 'cat' and 'dog':\")\n",
    "\n",
    "print(torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because 'cat' and 'dog' are semantically similar, their embeddings being close in the vector space helps our models generalize better, even if the word 'dog' didn't appear frequently in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the cosine similarity? \n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Returns the cosine of the angle between vectors `u` and `v`.\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.dot(u, v) / (torch.norm(u) * torch.norm(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating cosine similarity between word embeddings, we measure the angle between their corresponding vectors.\n",
    "\n",
    "**Vectors and angles**:  Imagine each word's embedding as a vector in a high-dimensional space.  The cosine similarity tells us how much these vectors point in the same direction.\n",
    "\n",
    "Similarity:\n",
    "\n",
    "* Cosine similarity ranges from -1 to 1.\n",
    "* A value of 1 means the vectors are perfectly aligned (angle = 0 degrees), indicating very similar words.\n",
    "* A value of 0 means the vectors are orthogonal (angle = 90 degrees), suggesting no relationship.\n",
    "* A value of -1 means the vectors point in opposite directions (angle = 180 degrees), implying antonyms or highly dissimilar words.\n",
    "\n",
    "> **Key takeaway**:  Cosine similarity provides a way to quantify semantic relationships between words based on the geometry of their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog 2.681130886077881\n",
      "bike 6.0225629806518555\n",
      "kitten 4.454165935516357\n",
      "puppy 3.9275598526000977\n",
      "kite 5.85930061340332\n",
      "computer 6.960630893707275\n",
      "neuron 7.568032264709473\n"
     ]
    }
   ],
   "source": [
    "# Measuring distance from a given word to a list of other words\n",
    "word = 'cat'\n",
    "other = ['dog', 'bike', 'kitten', 'puppy', 'kite', 'computer', 'neuron']\n",
    "\n",
    "for w in other:\n",
    "    dist = torch.norm(glove[word] - glove[w]) # euclidean distance\n",
    "    print(w, float(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, glove, top_n=10):\n",
    "    \"\"\"Finds the top_n most similar words to the given word using GloVe embeddings.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input word.\n",
    "        glove: A GloVe object containing word embeddings.\n",
    "        top_n (int, optional): The number of similar words to return. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings representing the most similar words.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        word_embedding = glove.get_vecs_by_tokens(word)\n",
    "        similarities = torch.cosine_similarity(glove.vectors, word_embedding)\n",
    "        top_n_similar = torch.topk(similarities, k=top_n)\n",
    "        top_n_words = [glove.itos[i] for i in top_n_similar.indices]\n",
    "        return top_n_words\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Word '{word}' not found in the GloVe vocabulary.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'cat', 'dogs', 'pet', 'puppy', 'horse', 'animal', 'pig', 'boy', 'cats']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words(\"dog\", glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer',\n",
       " 'computers',\n",
       " 'software',\n",
       " 'technology',\n",
       " 'pc',\n",
       " 'hardware',\n",
       " 'internet',\n",
       " 'desktop',\n",
       " 'electronic',\n",
       " 'systems']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words(\"computer\", glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['norway',\n",
       " 'denmark',\n",
       " 'sweden',\n",
       " 'finland',\n",
       " 'iceland',\n",
       " 'norwegian',\n",
       " 'austria',\n",
       " 'switzerland',\n",
       " 'greece',\n",
       " 'germany']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar_words(\"norway\", glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dat255/lib/python3.11/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample words for visualization\n",
    "sample_words = ['hello', 'world', 'cat', 'dog', 'king', 'queen', 'man', 'woman', \n",
    "                'good', 'bad', 'run', 'jump', 'large', 'small'] \n",
    "\n",
    "sample_embeddings = glove.get_vecs_by_tokens(sample_words)\n",
    "\n",
    "# Convert the embeddings to a NumPy array \n",
    "sample_embeddings_np = sample_embeddings.numpy()\n",
    "\n",
    "# Reduce dimensionality with t-SNE\n",
    "X_embedded = TSNE(n_components=2, perplexity=4).fit_transform(sample_embeddings_np)\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1])\n",
    "\n",
    "# Add labels for individual words \n",
    "for i, word in enumerate(sample_words):\n",
    "    plt.annotate(word, xy=(X_embedded[i, 0], X_embedded[i, 1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the visualization, notice how semantically related words tend to cluster together while unrelated words are farther apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also the interactive demonstrations we used in class based on http://projector.tensorflow.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned embeddings\n",
    "\n",
    "Learned embeddings are embeddings that are trained from scratch on a specific task. For example, when training a language model, the model learns to predict the next word in a sentence, and in the process, it learns to represent words as dense vectors. This allows embeddings to be fine-tuned specifically for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding vectors from skratch using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _MORE TBA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
