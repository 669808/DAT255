{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0293846",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Objectives</a></span></li></ul></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-data-into-PyTorch\" data-toc-modified-id=\"Load-data-into-PyTorch-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load data into PyTorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inspecting-the-data\" data-toc-modified-id=\"Inspecting-the-data-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Inspecting the data</a></span></li></ul></li><li><span><a href=\"#Data-loaders\" data-toc-modified-id=\"Data-loaders-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Data loaders</a></span></li></ul></li><li><span><a href=\"#A-simple-neural-network-in-PyTorch\" data-toc-modified-id=\"A-simple-neural-network-in-PyTorch-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>A simple neural network in PyTorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-fully-connected-neural-net-in-PyTorch\" data-toc-modified-id=\"A-fully-connected-neural-net-in-PyTorch-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>A fully-connected neural net in PyTorch</a></span></li></ul></li><li><span><a href=\"#Training-the-network\" data-toc-modified-id=\"Training-the-network-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Training the network</a></span><ul class=\"toc-item\"><li><span><a href=\"#1:-Initialization\" data-toc-modified-id=\"1:-Initialization-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>1: Initialization</a></span></li><li><span><a href=\"#2:-Grab-a-batch-and-predict\" data-toc-modified-id=\"2:-Grab-a-batch-and-predict-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>2: Grab a batch and predict</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feeding-the-batch-to-the-network\" data-toc-modified-id=\"Feeding-the-batch-to-the-network-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Feeding the batch to the network</a></span></li></ul></li><li><span><a href=\"#3:-Calculate-loss\" data-toc-modified-id=\"3:-Calculate-loss-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>3: Calculate loss</a></span></li><li><span><a href=\"#4:-Calculate-gradients\" data-toc-modified-id=\"4:-Calculate-gradients-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>4: Calculate gradients</a></span></li><li><span><a href=\"#5:-Step-/-update-the-weights\" data-toc-modified-id=\"5:-Step-/-update-the-weights-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>5: Step / update the weights</a></span><ul class=\"toc-item\"><li><span><a href=\"#Did-that-help?\" data-toc-modified-id=\"Did-that-help?-5.5.1\"><span class=\"toc-item-num\">5.5.1&nbsp;&nbsp;</span>Did that help?</a></span></li></ul></li><li><span><a href=\"#6:-Go-back-to-step-2-and-repeat\" data-toc-modified-id=\"6:-Go-back-to-step-2-and-repeat-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>6: Go back to step 2 and repeat</a></span></li><li><span><a href=\"#7:-Iterate-until-stopping-criterion-is-met\" data-toc-modified-id=\"7:-Iterate-until-stopping-criterion-is-met-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>7: Iterate until stopping criterion is met</a></span></li><li><span><a href=\"#Evaluate-the-results\" data-toc-modified-id=\"Evaluate-the-results-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Evaluate the results</a></span></li></ul></li><li><span><a href=\"#Wrapping-up\" data-toc-modified-id=\"Wrapping-up-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Wrapping up</a></span></li><li><span><a href=\"#The-&quot;deep&quot;-in-deep-learning\" data-toc-modified-id=\"The-&quot;deep&quot;-in-deep-learning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>The \"deep\" in deep learning</a></span></li><li><span><a href=\"#Convolutional-neural-networks\" data-toc-modified-id=\"Convolutional-neural-networks-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Convolutional neural networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-CNN-in-PyTorch\" data-toc-modified-id=\"A-CNN-in-PyTorch-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>A CNN in PyTorch</a></span></li></ul></li><li><span><a href=\"#Extra:-inspecting-what-the-model-has-learned\" data-toc-modified-id=\"Extra:-inspecting-what-the-model-has-learned-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Extra: inspecting what the model has learned</a></span></li><li><span><a href=\"#Extra:-using-fastai\" data-toc-modified-id=\"Extra:-using-fastai-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Extra: using fastai</a></span><ul class=\"toc-item\"><li><span><a href=\"#Get-the-CIFAR10-data\" data-toc-modified-id=\"Get-the-CIFAR10-data-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Get the CIFAR10 data</a></span></li><li><span><a href=\"#Create-dataloaders\" data-toc-modified-id=\"Create-dataloaders-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Create dataloaders</a></span></li><li><span><a href=\"#Train-model\" data-toc-modified-id=\"Train-model-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Train model</a></span></li><li><span><a href=\"#Inspect-results\" data-toc-modified-id=\"Inspect-results-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;</span>Inspect results</a></span></li><li><span><a href=\"#Extra-extra:-adding-a-few-tricks\" data-toc-modified-id=\"Extra-extra:-adding-a-few-tricks-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;</span>Extra extra: adding a few tricks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-a-different-(pre-trained)-architecture\" data-toc-modified-id=\"Using-a-different-(pre-trained)-architecture-10.5.1\"><span class=\"toc-item-num\">10.5.1&nbsp;&nbsp;</span>Using a different (pre-trained) architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-dataloaders\" data-toc-modified-id=\"Create-dataloaders-10.5.1.1\"><span class=\"toc-item-num\">10.5.1.1&nbsp;&nbsp;</span>Create dataloaders</a></span></li><li><span><a href=\"#Testing-a-few-architectures\" data-toc-modified-id=\"Testing-a-few-architectures-10.5.1.2\"><span class=\"toc-item-num\">10.5.1.2&nbsp;&nbsp;</span>Testing a few architectures</a></span></li></ul></li></ul></li><li><span><a href=\"#Ensembling\" data-toc-modified-id=\"Ensembling-10.6\"><span class=\"toc-item-num\">10.6&nbsp;&nbsp;</span>Ensembling</a></span></li></ul></li><li><span><a href=\"#Your-turn!\" data-toc-modified-id=\"Your-turn!-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Your turn!</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44100d61",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 29.01.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef348a8-18cc-431b-b6d8-0970f7c2f9cc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70642d1-b27c-4588-8539-05d6c08daacf",
   "metadata": {},
   "source": [
    "This quick tutorial introduces some basic building blocks of artificial neural networks and how they are constructed and trained in PyTorch by stepping through the training of a simple neural network. This is meant to provide a solid understanding of the basic steps of constructing and training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc198a3-a042-4220-8e0a-6a1b01065e3d",
   "metadata": {},
   "source": [
    "> Note that there were slides accompanying this notebook. We went through these in class. You'll find a copy on the course website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1400a-b7b5-4fbf-ad4f-bceac4e0e873",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Objectives\n",
    "1. A better understanding of what neural networks are and how they're trained. \n",
    "2. Some familiarity with using PyTorch to construct and train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69478538-e765-4152-84c5-ac05bf146039",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfe04a-7650-4008-8f0c-e4eb0d4ba67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ef235-68a7-4b79-aeba-e9510a95cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb68af-895d-4927-b784-ec9764775c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29b4bd-5fd2-4501-b5ff-5effe84da63c",
   "metadata": {},
   "source": [
    "Set up data directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf9744-bb11-4bd2-a220-4b914e3fe87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_DIR = Path.cwd()\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    DATADIR = Path(\"/content/gdrive/MyDrive/Colab Notebooks/data\")\n",
    "    DATADIR.mkdir(exist_ok=True)\n",
    "if not colab:\n",
    "    DATADIR = Path.home()/'data' # You can change this if you want to store data elsewhere\n",
    "    DATADIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f18ee9-c797-4144-8bda-206d087a19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b6296-5a5a-46a2-9e34-82c38f4c2a7a",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a94b7f-58af-433e-bd71-3fb28f7eb275",
   "metadata": {},
   "source": [
    "We'll use a famous benchmark data set, widely studied by the computer vision community: [Cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "It consists of 60.000 32x32 color images from 10 different classes:\n",
    "\n",
    "<img src=\"https://github.com/HVL-ML/DAT255/raw/main/nbs/assets/cifar10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d76afd-c194-40d4-a882-277d99e9f5b0",
   "metadata": {},
   "source": [
    "Our goal is to create an **image classifier**. That is, a model that can take an image as input and predict which of the 10 classes it belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1a1f8-0e34-4c59-af62-6dc903acf4b6",
   "metadata": {},
   "source": [
    "## Load data into PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537d339-2f3b-4b60-b435-1aca5b0d6d5b",
   "metadata": {},
   "source": [
    "PyTorch (and other deep learning frameworks) operate on what's called tensors, which are essentially multidimensional arrays that can be placed on GPUs for accellerated computing. We'll take a closer look at tensor and tensor operations in the next notebook.\n",
    "\n",
    "To process the Cifar-10 images, we need to convert them to tensors. We'll do that as we load the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for mean and standard deviation\n",
    "MEAN = (0.5, 0.5, 0.5)\n",
    "STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "# Define the transformation to be applied on the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),  # Normalize each channel with the defined mean and std\n",
    "])\n",
    "\n",
    "# Download and load the training and test datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root=DATADIR, train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root=DATADIR, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9062686e-4a1c-4282-bf4e-3872ac440348",
   "metadata": {},
   "source": [
    "### Inspecting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba35aa8-46e4-4785-823d-71bb39ee4c3c",
   "metadata": {},
   "source": [
    "Now we have a training data set with 50.000 images and a test data set with 10.000 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ac312-7c60-4da9-83b6-27fba88eafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f421d7e-9a03-41b5-aa91-9c378d657440",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8b421-fe2d-45ab-88f0-8fea5bd028af",
   "metadata": {},
   "source": [
    "The images are stored as 32x32x3 tensors. The second and third dimensions are the height and width of the image in pixels, and the last dimension is the number of color channels (3 for RGB images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7d0da-cc3e-40e8-94a1-0b3aeb0fc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.data.shape, testset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1d57d-7e03-45ee-b1bd-190c77bcfe76",
   "metadata": {},
   "source": [
    "Each image is labelled with one of 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae31b38-af6a-437a-b6df-84114a4f6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a452f-e0a0-47f1-95ce-0d14042c7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2dc366-c248-478e-b6ac-1d3579e44651",
   "metadata": {},
   "source": [
    "Here are the first few labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b0aaf-40a1-4c8a-b2f2-3b0f6251f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.targets[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f1abf-538c-48b5-b7b0-79be346c85f3",
   "metadata": {},
   "source": [
    "...corresponding to the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1ee2e-7e60-45b2-9d12-b8739771f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.class_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136c2eaa-1fc4-4b08-939b-daa87639edf9",
   "metadata": {},
   "source": [
    "Let's open an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ef37a-57ff-4992-9b6f-7caef37f2b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img = trainset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bbcb4-0d5f-4a78-b468-673a1ec688a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ad1ada-5577-4546-89c5-3bd6fa81d116",
   "metadata": {},
   "source": [
    "Here's a small section of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcad1d9-b50a-449b-9174-b74851b1fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img[:2,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b499fd6-3569-4fb7-a5e3-d094b2fbc426",
   "metadata": {},
   "source": [
    "We see that they are NumPy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2f836-bb67-40d3-8148-1c02e2555d41",
   "metadata": {},
   "source": [
    "Applying the specified transforms we obtain tensors, scaled using `Normalize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879c3fd-8e23-4362-9f47-11a91a5f2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.transform(example_img[:2,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d11c9-285b-47f5-9687-51eee385ba32",
   "metadata": {},
   "source": [
    "We can plot the image using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d009da-7162-4cfb-a128-09871f3d2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8aa7e4-7723-4e6c-83d8-da8e7fffbced",
   "metadata": {},
   "source": [
    "Looks like a frog.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ea659-85c0-44e4-809e-35767cb684cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.targets[0], trainset.class_to_idx['frog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd4eb9-a438-463b-aa8d-9027f94170e9",
   "metadata": {},
   "source": [
    "Yup, that's a frog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bab250-6cd1-4d13-a62b-09b0406cab1b",
   "metadata": {},
   "source": [
    "Aside: it'll be useful to have an `idx_to_class` dict as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8211a-4448-4cd4-a91d-75b5b87456d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {v:k for k, v in trainset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97ef40-8713-48f8-9188-0ce94196394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeca6388-cf50-4fa0-a4b3-52fbff46f411",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf626c7a-1fb0-4b7d-b934-0ce627a1b81c",
   "metadata": {},
   "source": [
    "As we've seen, we'd like to feed _batches_ of these tensors onto the GPU when training a neural network. ***Data loaders*** can take care of that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e06dec-4bcd-4fe0-bc9d-d051df928780",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b7bed-a298-41d8-9b4a-ca0f60337e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,  shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01926161-a05b-405d-9071-2a661600e641",
   "metadata": {},
   "source": [
    "It's good practice to check that our data loaders returns what we expect. In the case of images, we can plot a batch of images to see that they look right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for normalization\n",
    "NORM_FACTOR = 2\n",
    "NORM_SHIFT = 0.5\n",
    "\n",
    "def plot_cifar10():\n",
    "    \"\"\"\n",
    "    Function to plot images from the CIFAR10 dataset.\n",
    "\n",
    "    The function takes no parameters and returns no values. It plots a grid of images from the CIFAR10 dataset.\n",
    "    \"\"\"\n",
    "    images, labels = next(iter(trainloader))\n",
    "    img = torchvision.utils.make_grid(images)\n",
    "    \n",
    "    # Undo the normalization for plotting purposes:\n",
    "    img = img.numpy() / NORM_FACTOR + NORM_SHIFT\n",
    "    \n",
    "    # We want the images are represented as (height, width, channel):\n",
    "    img = np.transpose(img, (1, 2, 0)) \n",
    "    \n",
    "    plt.figure(figsize=(18,8))\n",
    "    plt.axis(\"Off\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f'{[trainloader.dataset.classes[labels[j]] for j in range(batch_size)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5fe37-8bcf-4621-991a-ceb90470582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958aefc-2366-45df-8ceb-1287051ec7eb",
   "metadata": {},
   "source": [
    "# A simple neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b0286",
   "metadata": {},
   "source": [
    "As we discussed in the lecture, there are two main stages when training a neural network:\n",
    "\n",
    "- A **forward pass** to get values for all the nodes between the input and output.\n",
    "- A **backward pass** where backpropagation (based on [*automatic differentiation*](https://en.wikipedia.org/wiki/Automatic_differentiation) and [*dynamic programming*](https://en.wikipedia.org/wiki/Dynamic_programming)) and gradient descent is used to tweak all the parameters in the network.\n",
    "\n",
    "To make gradient descent work, we need to be able to take the derivative of each component in the network (these derivatives are computed during backpropagation). Even though it's basically an application of the simple chain rule from calculus, it can be a bit complicated to do it efficiently. For details about these procedures, see [Stanford's CS231n notes on optimization](http://cs231n.github.io/optimization-2).\n",
    "\n",
    "Luckily, PyTorch can take care of the differentiation for us (that is, the backward pass) if we create a network inheriting from `nn.Module`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bfdf1-2fa0-4b18-8450-3058ff5a3e2c",
   "metadata": {},
   "source": [
    "## A fully-connected neural net in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344f55e-2044-4f87-8626-f4a64c204cd8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here's a simple one hidden layer neural network in PyTorch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5b905-2536-4c45-9a39-7f0bee87b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32*32*3 # The Cifar-10 images are 32x32 with three color channels\n",
    "hidden_size = 16 # We can choose this number\n",
    "num_classes = len(trainset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ddfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize the network with an input layer, a hidden layer, and an output layer.\n",
    "\n",
    "        Parameters:\n",
    "        input_size (int): The size of the input layer.\n",
    "        hidden_size (int): The size of the hidden layer.\n",
    "        num_classes (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # A hidden layer. A tensor of size  (hidden_size) x (input_size)\n",
    "        self.hidden_layer = torch.nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # An output layer. A tensor of size (num_classes) x (hidden_size)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    # We need to define what we want to happen in the forward phase (the backward phase is automatic)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the network.\n",
    "\n",
    "        Parameters:\n",
    "        x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        x (Tensor): The output tensor.\n",
    "        \"\"\"\n",
    "        # Make the 32x32x3 image into a 32*32*3 = 3072 vector\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Feed the input vector through the hidden layer and an activation function\n",
    "        x = torch.tanh(self.hidden_layer(x))\n",
    "        \n",
    "        # Output num_classes of numbers.\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2413fd1-dc7c-4fe5-acba-8a4ebd7166ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size = input_size, hidden_size = hidden_size, num_classes = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951a230-348b-4a6f-9474-d0a6e5290315",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ce62f-cb85-4a4a-aae1-60798c3460cf",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1be6841-d19b-48d2-b59b-7aa16f4d8d79",
   "metadata": {},
   "source": [
    "There are seven steps to train a neural network (this is a slight reformulation of https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a3887-1112-4fa6-8407-f4ad0301099d",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/HVL-ML/DAT255/raw/main/2-DL_building_blocks/assets/fastai-train_nnet_figure.png\"><br>\n",
    "<center><small>Figure from the <a href=\"https://github.com/fastai/fastbook\">fastai book.</a></small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9bf249-3228-4ddb-9500-c94c838e269c",
   "metadata": {},
   "source": [
    "1. _Initialize_ the parameters.\n",
    "2. Grab a batch of inputs and use the model with its current parameters to make _predictions_.\n",
    "3. Based on these predictions, calculate how good the model is (its _loss_).\n",
    "4. Calculate the _gradients_ for each parameter using backpropagation. This measures how changing each parameter would change the loss\n",
    "5. _Step_ (that is, change) all the parameters based on that calculation.\n",
    "6. Go back to step 2, and _repeat_ the process.\n",
    "7. Iterate until you decide to _stop_ the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc75b44-d18e-492b-a77a-53bfca639176",
   "metadata": {},
   "source": [
    "## 1: Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e37451-3533-43cf-a0b9-2b6c66e610cb",
   "metadata": {},
   "source": [
    "PyTorch has taken care of the initalization for us by choosing random numbers for the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb9e5b-baa1-41cc-82ff-1310ffea16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89181269-c6c0-4a60-9b98-d76374d8fff4",
   "metadata": {},
   "source": [
    "## 2: Grab a batch and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f34c49-9ec6-4810-927e-9f9616ab46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch\n",
    "i, data = next(enumerate(trainloader, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d56cb5-a820-4f58-a303-03152c4984b2",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80ddab-523f-478d-aeea-29b831674e95",
   "metadata": {},
   "source": [
    "We expect to have collected the first batch of 16 images and their labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35633da-2517-4c93-856a-1afa0d47ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446cd11-dcb9-469f-9e43-f5b110dc4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4c236-3650-4f29-b343-0d58306fcfb6",
   "metadata": {},
   "source": [
    "Their labels are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3c756-915c-4648-a2b7-98a46084b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95ba01-e5ef-4498-be2d-6b1d621c5969",
   "metadata": {},
   "source": [
    "...which means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca206891-ed15-48c3-a72c-81633c990a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [idx_to_class[int(l)] for l in labels]\n",
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c3d762-4503-4de4-9a82-253f9a0f593c",
   "metadata": {},
   "source": [
    "The images are of course torch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42231176-692f-44bc-a200-3caea918cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6dd96-ad8d-4c05-b74f-d31e21734f66",
   "metadata": {},
   "source": [
    "16 images, 3 color channels, dimensions 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de363c7f-3d60-4ce8-9b7e-ee703da28f14",
   "metadata": {},
   "source": [
    "### Feeding the batch to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc827e88-8190-44f9-8e81-0d9f19388225",
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05cab4-51d0-4cb1-8324-db78ca6bf97d",
   "metadata": {},
   "source": [
    "Computing the predictions from the network for these images is simple, as the network can be treated as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00197cb5-fcc2-4ce9-8f95-9c0ea786a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288f3532-5f6a-4477-99c7-28ec53c78d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54bae8-65e3-4b39-89d5-e70abe79bc18",
   "metadata": {},
   "source": [
    "We obtained 16 predictions of 10 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44543bf0-89b2-49ae-a69f-3ea53e087b21",
   "metadata": {},
   "source": [
    "Here's the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51babcff-92fb-451a-953d-da18bef1949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c467d5f-44ae-461f-bdb4-96f631dc230a",
   "metadata": {},
   "source": [
    "> Our goal is to make the element in each vector that corresponds to the correct label for the corresponding image as large as possible, while the rest are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d649759e-0954-4556-bc2d-aee34018d74d",
   "metadata": {},
   "source": [
    "Since we haven't trained our network at all yet, we can't expect it to produce any good predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9122fe-cfd0-45c3-8d26-f7202e034578",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_classes = torch.max(predictions.data, 1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd76afe-f9a1-4cfd-8368-3c47e316c433",
   "metadata": {},
   "source": [
    "We can compare this to the correct outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b26878-5379-493e-82b5-282efeacb5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx_to_class[int(p)] for p in predicted_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13055f36-36c3-4261-adfa-2b8db6450584",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad51ec-a7b7-48d7-b1e9-98054b292bd0",
   "metadata": {},
   "source": [
    "Chances are that very few of the predictions will be correct. This is because we've initialized the network with random weights, and it hasn't been trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0f924-e027-42a2-b726-a167c2537fa7",
   "metadata": {},
   "source": [
    "## 3: Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470be46c-41d2-4d3b-b3b7-8326cfeb24b1",
   "metadata": {},
   "source": [
    "To measure the discrepancy between the predictions and the true labels we need a loss function suited for classification tasks. The **cross-entropy loss** fits the bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24596e96-4155-43c2-85d7-7b1de35de4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25909cd5-97b1-4329-89aa-8f4313e44ea4",
   "metadata": {},
   "source": [
    "Let's calculate the loss for our current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28110d-b1ef-4841-ad38-aacfda622764",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_loss = loss_func(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dff29c-446a-40cb-a156-a9b7f1159f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b20fd5-ab0d-43e8-ac3b-4a03cd2baa9c",
   "metadata": {},
   "source": [
    "## 4: Calculate gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b5b01-c471-4393-94c0-196a4512436e",
   "metadata": {},
   "source": [
    "Using the `backward` method PyTorch can calculate how much each weight in the network contributed to the loss by calculating the gradient of the loss with respect to each of them (using a technique called automatic differentation, which is related to the chain rule you know from calculus):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a947dc6-28d2-4617-a0f4-ef3ddd542eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02b1726-d2f8-4af6-b817-064712d6f0c3",
   "metadata": {},
   "source": [
    "## 5: Step / update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab97779-7367-484a-9302-0e3d4d3ae59f",
   "metadata": {},
   "source": [
    "Now we can update all the weights using gradient descent. For that we need to set a learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e454ef-4c97-4423-8411-a55755d88612",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f5663-33c4-427c-9852-b170bb5abce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6dded-a2a2-4647-b20d-f87e7261476a",
   "metadata": {},
   "source": [
    "We'll use a built-in stochastic gradient descent optimizer from PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e30ede-43ee-467f-bcf8-5073533c7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae6670-b241-4faa-b2f6-6a6b82710ab1",
   "metadata": {},
   "source": [
    "> Aside: Check source code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b1c4e9-2530-4eb9-8aba-255de48a39d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Take one step with the optimizer to modify each weight:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f48e0d-14e3-4653-bce3-dd30c92712d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b727ae4-dce5-40b0-8f3e-29040cf8568b",
   "metadata": {},
   "source": [
    "The network has now been trained _a tiny bit_ (it has learned from the first 16 images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fed586-75e2-47ee-abdb-0707213bd8fd",
   "metadata": {},
   "source": [
    "### Did that help?\n",
    "\n",
    "Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040dafd3-21e8-438d-a851-47486a976973",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = net(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc153c5a-2418-4d56-bbd3-705a07e37848",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted_classes = torch.max(predictions.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91cf3b-e836-4767-a292-d0754a3c329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx_to_class[int(p)] for p in predicted_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b392f2d-cfe5-4288-962a-1a37c2f98598",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f0656-426b-40da-bb4e-b3674618cde1",
   "metadata": {},
   "source": [
    "The model imporved, but not by much. Can also see that the loss didn't decrease significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b595f71-f574-484a-82be-3bcdd0ab17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae44621-e6c6-4d1f-8aef-2473f1345016",
   "metadata": {},
   "source": [
    "## 6: Go back to step 2 and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329a37a-ef3a-4072-bae4-5e8acbe6940d",
   "metadata": {},
   "source": [
    "The idea now is to repeat this batch by batch until we've been through the entire training data set multiple times. The idea is that gradient descent will then be able to find good settings for all the weights, and we can use those when new data is fed through the network.\n",
    "\n",
    "Let's first create a simple function for checking the accuracy of the network on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(net, dataloader):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a network on a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    net (torch.nn.Module): The network to test.\n",
    "    dataloader (torch.utils.data.DataLoader): The dataset to test the network on.\n",
    "\n",
    "    Returns:\n",
    "    accuracy (float): The accuracy of the network on the given dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Feed all the test data through the net and count the number of correct predictions:\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        predictions = net(images)\n",
    "        _, predicted_classes = torch.max(predictions.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted_classes == labels).sum()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'The accuracy of the network on the {total} test images is: {100 * accuracy:.2f} %')\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dec6f0-4d86-4299-921d-4b4a9b461f62",
   "metadata": {},
   "source": [
    "Our current accuracy after training on a single batch is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686c2a5-d560-4258-b4a8-47e10347b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_accuracy(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49543776-e0c8-4cdb-9b42-62504f9779a7",
   "metadata": {},
   "source": [
    "Since the network is essentially untrained this is as expected (randomly guessing among the 10 classes gives an accuracy of 10%).\n",
    "\n",
    "The cell below the optimizer initalization contains the entire above procedure. You can CTRL+ENTER this cell to run it multiple times. You're training a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686cc023-129b-4e4e-a6fe-697009ce0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size = input_size, hidden_size = hidden_size, num_classes = num_classes)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffd993-7d69-4e9c-9581-ddd86b272549",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "# 2. Grab the next batch and compute predictions\n",
    "###################################################\n",
    "\n",
    "i, data = next(enumerate(trainloader, 0))\n",
    "images, labels = data\n",
    "# A technicality: we have to zero out the gradients each time, \n",
    "# otherwise they'll accumulate\n",
    "optimizer.zero_grad()\n",
    "predictions = net(images)\n",
    "\n",
    "\n",
    "###################################################\n",
    "# 3. Compute the loss\n",
    "###################################################\n",
    "\n",
    "loss = loss_func(predictions, labels)\n",
    "print(f'Current loss: {loss}')\n",
    "\n",
    "\n",
    "###################################################\n",
    "# 4. Calculate the gradients\n",
    "###################################################\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "###################################################\n",
    "# 5. Step\n",
    "###################################################\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "# Compute the current accuracy\n",
    "_ = check_accuracy(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815f737-c04f-4ebe-bef6-997cd668559d",
   "metadata": {},
   "source": [
    "> **Question:** How many times do you have to run the above cell to go through the entire training data set once?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde470ad-a542-42bf-b07c-6f5d06cc0cee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's make a loop!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c8c25-e512-4e4f-ad06-edd164f9f7ba",
   "metadata": {},
   "source": [
    "## 7: Iterate until stopping criterion is met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281c964-8e06-4462-9f04-4fc34c05dcce",
   "metadata": {},
   "source": [
    "Stopping criterion: We'll go through the entire training set 3 times. Feel free to decrease or increase this (training will take quite some time if running on a CPU, not a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb31fc9-d714-480e-8903-277eada35ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We record the accuracies during training for later analysis\n",
    "accuracies = [check_accuracy(net, testloader), ]\n",
    "\n",
    "# Define constant for print interval\n",
    "PRINT_INTERVAL = 1000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        predictions = net(inputs)\n",
    "        loss = loss_func(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data\n",
    "        if i % PRINT_INTERVAL == PRINT_INTERVAL - 1:    # print every PRINT_INTERVALth batch         \n",
    "            print(f'[{epoch + 1}, {i + 1}] loss: {running_loss / PRINT_INTERVAL:.3f}')\n",
    "            \n",
    "            acc = check_accuracy(net, testloader)\n",
    "            \n",
    "            print(\"-\"*40)\n",
    "            accuracies.append(acc)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ee6aa-bb37-41f0-8ba2-e2e2a21909a7",
   "metadata": {},
   "source": [
    "Here's a plot of the accuracies obtained during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9747c6-97d7-4f1f-8766-9bd242f100fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylim([0.1, 0.6])\n",
    "plt.plot(accuracies, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe40df-a76f-43b8-b854-94e06f245d44",
   "metadata": {},
   "source": [
    "We see that we quickly reach what seems to be a plateau in accuracy. We need something else to go beyond this..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e1566-8d30-4f08-8228-6e5dadbe7dc4",
   "metadata": {},
   "source": [
    "## Evaluate the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac3d83-5e99-4922-b10c-4eba0c4a4fd1",
   "metadata": {},
   "source": [
    "As you know from basic machine learning, one should always turn to the **confusion matrix** when evaluating a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f96f79-5873-4377-909b-33168a16ceb5",
   "metadata": {},
   "source": [
    "First we collect lists of the predicted labels for the test data and the true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab64b1-8ef6-4692-95c7-8c9d47af6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        predictions = net(images)\n",
    "        _, predicted_classes = torch.max(predictions.data, 1)\n",
    "        y_pred.append(list(predicted_classes.numpy()))\n",
    "        y_true.append(list(labels.numpy()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6c013-dd53-4246-a55c-8da7c4d35de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the lists (they are lists of sublists of length batch size)\n",
    "y_pred = [i for sublist in y_pred for i in sublist]\n",
    "y_true = [i for sublist in y_true for i in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a6fab0-4f69-4946-9bc1-94260a57b922",
   "metadata": {},
   "source": [
    "...then compute the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fcaee-d9cc-4aba-8783-bb8c6dc934f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e127a-6c22-41e7-a980-fb6579018121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_cm = pd.DataFrame(cm, testset.classes, testset.classes)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, fmt=f'.0f', annot_kws={\"size\": 11}) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def5d2a5-2958-41e1-a041-d3fb9e630ce6",
   "metadata": {},
   "source": [
    "# Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872e63d-026d-4881-96b7-7ae02ec495d9",
   "metadata": {},
   "source": [
    "**That's it, basically**! Now you know the basics of training neural networks from skratch.\n",
    "\n",
    "However, our above neural network is not particularly powerful. One can, of course, do much, much better...\n",
    "\n",
    "> **Your turn!** Try adding a second hidden layer to the network. How does it influence performance? Try experimenting with the sizes of the hidden layers. Are you able to obtain a better result than the above? Note: if you remove the \"check_accuracy\" call in the training loop, it'll speed up the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae101f",
   "metadata": {},
   "source": [
    "> **Your turn!** Create a network that includes a _dropout layer_ (used for regularization). \n",
    "\n",
    "> **Your turn!** Create a network with a different activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f66121",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for a hint for the dropout layer exercise</summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(1024, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 256)\n",
    "        # TODO: Add a dropout layer with p=0.5 here\n",
    "        self.fc3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        # TODO: Apply dropout here\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75dfaa2-0d31-4a76-839b-8434b091e3b1",
   "metadata": {},
   "source": [
    "# The \"deep\" in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c09b21-9eda-4ca3-8e32-631a76a11d31",
   "metadata": {},
   "source": [
    "Fundamentally, *deep learning* for neural networks simply means adding more than one hidden layer: i.e., making the network *deep*. If you modify the above neural network definition by adding a second hidden layer, you're doing deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f4a7a-e880-4245-bbea-39007b6ab97d",
   "metadata": {},
   "source": [
    "However, \"deep learning\" also refers to an entire field of research, chock-full of exciting ideas, models, and techniques, ranging from the [theoretical](https://geometricdeeplearning.com/) to the [practical](https://developers.google.com/machine-learning/guides/rules-of-ml). As you've seen in the lectures, it's an extremely hot field that exploded a few years ago and is still growing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb1929-2282-4910-8054-de9e39f7da66",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3b1ed",
   "metadata": {},
   "source": [
    "In the above example, we constructed a simple **fully-connected** neural network and applied it to an image classification task. However, that is not a reasonable approach if you care about performance, as the geometric structure of the image isn't utilized at all. \n",
    "\n",
    "For example, using the naive representation of an image as a (long) 1-dimensional vector with a fully-connected neural network, as we did above, means that we have no efficient way to deal with the built-in *shift-invariance* of an image classification problem: objects in images are the same even if moved around. But if we shift the entire image by a single pixel, the vector representation will be very different, and each neuron in the neural network will get very different inputs. \n",
    "\n",
    "This is solved by the _weight-sharing_ of **convolutional neural networks** and their translation invariance, making them one of the most powerful types of neural networks for image analysis.\n",
    "\n",
    "Let's build one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f1443-6f7c-4034-bdbd-bc93d59bfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width=60% autoplay loop> <source src=\"https://github.com/HVL-ML/DAT255/raw/main/nbs/assets/CNN-viz-otavio-good.mp4\"> </video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e28c0-3172-4652-8a5b-6392cd7e5458",
   "metadata": {},
   "source": [
    "## A CNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c22ab-6910-406c-a6e8-000642005433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting some parameters for the model\n",
    "kernel_size = 5\n",
    "out_channels = [10, 20]\n",
    "fc_features = [320, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17d41b-58ad-4f98-a976-efb9900c3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolution and pooling:\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=3, \n",
    "                                     out_channels=out_channels[0], \n",
    "                                     kernel_size=kernel_size)\n",
    "        \n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=out_channels[0], \n",
    "                                     out_channels=out_channels[1], \n",
    "                                     kernel_size=kernel_size)\n",
    "        \n",
    "        \n",
    "        # Some fully-connected layers at the end:\n",
    "        self.fc1 = torch.nn.Linear(in_features=500, \n",
    "                                   out_features=fc_features[0])\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(in_features=fc_features[0], \n",
    "                                   out_features=fc_features[1])\n",
    "        \n",
    "        # Output layer:\n",
    "        self.fc3 = torch.nn.Linear(in_features=fc_features[1], \n",
    "                                   out_features=num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734145d-47bf-434b-8ef1-3a6595998429",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2f934-a962-44f6-8e66-fc8c52298a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610da9a8-709c-4e71-baf2-8a61b57c5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e8ee3-ccfd-40dd-b1f5-46e4291887e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec256b9-0ddf-4b8c-99ec-eec62161ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "for epoch in range(num_epochs): \n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data\n",
    "        if i % 1000 == 999:    # print every 1000nd batch         \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4811f33-55f0-4f5c-968e-872b87137fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = check_accuracy(cnn, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f6acf7-0ec0-47ff-b94a-e6adb6b76ada",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58e9f8-5901-4495-8d2f-c07091396f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        predictions = cnn(images)\n",
    "        _, predicted_classes = torch.max(predictions.data, 1)\n",
    "        y_pred.append(list(predicted_classes.numpy()))\n",
    "        y_true.append(list(labels.numpy()))        \n",
    "\n",
    "y_pred = [i for sublist in y_pred for i in sublist]\n",
    "y_true = [i for sublist in y_true for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf25655-8864-4289-b274-ee12b627174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, testset.classes, testset.classes)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cm, annot=True, fmt=f'.0f', annot_kws={\"size\": 11}) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62de1de-65dd-4ef3-b23a-3a7a7fad814c",
   "metadata": {},
   "source": [
    "> **Your turn!** Try playing with the parameters we chose for our CNN. The kernel_size, the number of output channels (which is the width of the network), etc. You can also try adding another convolutional layer to the network to check whether that improves the result. Note that the training time will increase as you increase the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8296ef4-ca4e-4fe2-91bf-edab09679174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trainable parameters for the current CNN model:\n",
    "sum(p.numel() for p in cnn.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c20ff8",
   "metadata": {},
   "source": [
    "# Extra: inspecting what the model has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de70c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_maps(model, input_image, layer_name):\n",
    "    \"\"\"Plot the feature maps of a specific layer for a given input image.\"\"\"\n",
    "    model.eval()\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(m, i, o):\n",
    "        \"\"\"Hook function to capture the outputs of the layer.\"\"\"\n",
    "        activations.append(o)\n",
    "\n",
    "    # Check if the layer exists in the model\n",
    "    if not hasattr(model, layer_name):\n",
    "        raise ValueError(f'No such layer: {layer_name}')\n",
    "\n",
    "    # Add hook to the specified layer\n",
    "    getattr(model, layer_name).register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        model(input_image)\n",
    "\n",
    "    # Get the output from the specified layer\n",
    "    conv_output = activations[0]\n",
    "    \n",
    "    num_features = conv_output.shape[1]\n",
    "    fig, axes = plt.subplots(1, num_features, figsize=(20, 5))\n",
    "    for i in range(num_features):\n",
    "        axes[i].imshow(conv_output[0, i].detach().cpu().numpy(), cmap=\"gray\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b18480",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61275a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd216ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_maps(cnn, images[0].unsqueeze(0), 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21066f",
   "metadata": {},
   "source": [
    "# Extra: using fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252ea52",
   "metadata": {},
   "source": [
    "The above story introduced the very basics of deep learning and image classification using deep learning. There's as you know a lot more to say about the topic. As a small taste, let's try to construct a more powerful image classification model using some more state-of-the-art techniques.\n",
    "\n",
    "> For this part of the story, it is important to have a CUDA-compatible NVIDIA GPU with updated drivers. If you're using Colab, make sure to attach a GPU to the runtime. If you're running on your own computer, make sure that you have PyTorch installed with CUDA enabled. See https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a169e4",
   "metadata": {},
   "source": [
    "Let's try to use the `fastai` library to construct and train our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272bfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip3 install -U fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259100c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40764cda",
   "metadata": {},
   "source": [
    "## Get the CIFAR10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df663695",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = untar_data(URLs.CIFAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a7bbd",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4cc352",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051afcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(DATADIR, train='train', valid='test', \n",
    "                                   batch_tfms=[*aug_transforms(), \n",
    "                                               Normalize.from_stats(*imagenet_stats)],\n",
    "                                    item_tfms=Resize(224), bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7be7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ef777",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf536d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = vision_learner(dls, arch=resnet18, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9290dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_lrs = learn.lr_find(suggest_funcs=(valley, steep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2c4d5-8e9d-4799-ae14-09e4867b3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = (suggested_lrs.valley + suggested_lrs.steep)/2\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(3, base_lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27985fd4",
   "metadata": {},
   "source": [
    "We obtain an accuracy of about 94.9%. That means that out of the 10.000 test images, only $(1-0.949)\\times 10000 = 510$ are misclassified by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b634d33",
   "metadata": {},
   "source": [
    "## Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7cb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fdabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd83724",
   "metadata": {},
   "source": [
    "Here are the images where the model was wrong and also confident (as measured by the loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495e610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(k=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ab6502",
   "metadata": {},
   "source": [
    "> If we add a few more tricks, like the ones you already know from fastai, the performance will increase even further. The CIFAR10 dataset turns out to be way too simple as a benchmark for modern computer vision methods based on deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c24ac9-0a8d-435e-ae20-58dbeef79c4c",
   "metadata": {},
   "source": [
    "## Extra extra: adding a few tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d66ba-57d0-4b10-90a6-c006470fe771",
   "metadata": {},
   "source": [
    "> Here are two examples of modifications to the above setup: modifying the architecture and ensembling multiple models. Other ideas include adding additional data agumentation and using test-time augmentation (TTA). Feel free to try out these and other techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9325b09-f48d-4efc-83ae-aa16d6d26289",
   "metadata": {},
   "source": [
    "### Using a different (pre-trained) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1815bf-88a7-4996-9f1e-88bfe7d98ddb",
   "metadata": {},
   "source": [
    "> You should either restart the Jupyter kernel or run the garbage collection below before continuing. Otherwise, you'll likely run out of GPU memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30cbed32-6fc4-4fd8-837a-09ce2203d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del learn\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84bd8180-5878-46a1-88e3-2fdd33d73de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0996f9b3-d618-408f-bbb7-0c9288aea384",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip3 install -U fastai\n",
    "    !pip3 install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21f0d7fa-b86e-4c46-ab9d-6025800d8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "289130e8-5612-41ee-89c3-a4374b1a53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e881c77-e5cd-4ba9-a546-dc09a7939c9f",
   "metadata": {},
   "source": [
    "#### Create dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9e49c-efcf-4458-b345-22297f5333e0",
   "metadata": {},
   "source": [
    "> We redo this in case you've restarted the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01acb2c4-9013-4590-a60f-4fe7e7c96b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = untar_data(URLs.CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e6d6f75-b7b7-4378-9a08-9a1ff27eba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(DATADIR, train='train', valid='test', \n",
    "                                   batch_tfms=[*aug_transforms(), \n",
    "                                               Normalize.from_stats(*imagenet_stats)],\n",
    "                                    item_tfms=Resize(224), bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08979b-7957-4a19-8074-788df3ab9400",
   "metadata": {},
   "source": [
    "#### Testing a few architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c898e-fb73-4f9d-8a35-04a1c19540be",
   "metadata": {},
   "source": [
    "> Note: Feel free to try out other architectures here. If you're primarily interested in ensembling, you may want to skip this section and jump to \"Ensembling\" below (as the training of each model takes some time).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85816047-23db-4fc9-9eca-c4f843890c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['convnext_atto',\n",
       " 'convnext_atto_ols',\n",
       " 'convnext_base',\n",
       " 'convnext_base_384_in22ft1k',\n",
       " 'convnext_base_in22ft1k',\n",
       " 'convnext_base_in22k',\n",
       " 'convnext_femto',\n",
       " 'convnext_femto_ols',\n",
       " 'convnext_large',\n",
       " 'convnext_large_384_in22ft1k',\n",
       " 'convnext_large_in22ft1k',\n",
       " 'convnext_large_in22k',\n",
       " 'convnext_nano',\n",
       " 'convnext_nano_ols',\n",
       " 'convnext_pico',\n",
       " 'convnext_pico_ols',\n",
       " 'convnext_small',\n",
       " 'convnext_small_384_in22ft1k',\n",
       " 'convnext_small_in22ft1k',\n",
       " 'convnext_small_in22k',\n",
       " 'convnext_tiny',\n",
       " 'convnext_tiny_384_in22ft1k',\n",
       " 'convnext_tiny_hnf',\n",
       " 'convnext_tiny_in22ft1k',\n",
       " 'convnext_tiny_in22k',\n",
       " 'convnext_xlarge_384_in22ft1k',\n",
       " 'convnext_xlarge_in22ft1k',\n",
       " 'convnext_xlarge_in22k']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('convnext*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "859e08b8-4f88-4d9b-a3d4-9312bc477eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swin_base_patch4_window7_224',\n",
       " 'swin_base_patch4_window7_224_in22k',\n",
       " 'swin_base_patch4_window12_384',\n",
       " 'swin_base_patch4_window12_384_in22k',\n",
       " 'swin_large_patch4_window7_224',\n",
       " 'swin_large_patch4_window7_224_in22k',\n",
       " 'swin_large_patch4_window12_384',\n",
       " 'swin_large_patch4_window12_384_in22k',\n",
       " 'swin_s3_base_224',\n",
       " 'swin_s3_small_224',\n",
       " 'swin_s3_tiny_224',\n",
       " 'swin_small_patch4_window7_224',\n",
       " 'swin_tiny_patch4_window7_224',\n",
       " 'swinv2_base_window8_256',\n",
       " 'swinv2_base_window12_192_22k',\n",
       " 'swinv2_base_window12to16_192to256_22kft1k',\n",
       " 'swinv2_base_window12to24_192to384_22kft1k',\n",
       " 'swinv2_base_window16_256',\n",
       " 'swinv2_cr_base_224',\n",
       " 'swinv2_cr_base_384',\n",
       " 'swinv2_cr_base_ns_224',\n",
       " 'swinv2_cr_giant_224',\n",
       " 'swinv2_cr_giant_384',\n",
       " 'swinv2_cr_huge_224',\n",
       " 'swinv2_cr_huge_384',\n",
       " 'swinv2_cr_large_224',\n",
       " 'swinv2_cr_large_384',\n",
       " 'swinv2_cr_small_224',\n",
       " 'swinv2_cr_small_384',\n",
       " 'swinv2_cr_small_ns_224',\n",
       " 'swinv2_cr_tiny_224',\n",
       " 'swinv2_cr_tiny_384',\n",
       " 'swinv2_cr_tiny_ns_224',\n",
       " 'swinv2_large_window12_192_22k',\n",
       " 'swinv2_large_window12to16_192to256_22kft1k',\n",
       " 'swinv2_large_window12to24_192to384_22kft1k',\n",
       " 'swinv2_small_window8_256',\n",
       " 'swinv2_small_window16_256',\n",
       " 'swinv2_tiny_window8_256',\n",
       " 'swinv2_tiny_window16_256']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm.list_models('swin*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ce6b5b3-f7aa-4eb7-b907-70c9fff43713",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures_to_test = [\n",
    "    \"convnext_tiny_in22k\",\n",
    "    \"convnext_small_in22k\",\n",
    "    \"swin_s3_tiny_224\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7712bd4-a406-4039-b038-66d7fbedc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(arch, epochs=3, savefn=None):\n",
    "    learn = vision_learner(dls, arch, metrics=accuracy).to_fp16()\n",
    "    learn.fine_tune(epochs)\n",
    "    \n",
    "    if savefn:\n",
    "        learn.export(f\"{savefn}.pkl\")\n",
    "        \n",
    "    # Garbage collection (we don't want to store the learner in memory)\n",
    "    del learn\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "700dc145-c34f-48b9-8408-447a952dd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_tiny_in22k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.179013</td>\n",
       "      <td>0.111463</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.102561</td>\n",
       "      <td>0.085725</td>\n",
       "      <td>0.972600</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060953</td>\n",
       "      <td>0.061643</td>\n",
       "      <td>0.979100</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>0.057546</td>\n",
       "      <td>0.981700</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "convnext_small_in22k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.128108</td>\n",
       "      <td>0.082298</td>\n",
       "      <td>0.976200</td>\n",
       "      <td>01:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.063210</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.043160</td>\n",
       "      <td>0.046098</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025256</td>\n",
       "      <td>0.044873</td>\n",
       "      <td>0.987400</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "swin_s3_tiny_224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/fastai/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.271912</td>\n",
       "      <td>0.155144</td>\n",
       "      <td>0.951700</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.170242</td>\n",
       "      <td>0.103604</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.114108</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.091466</td>\n",
       "      <td>0.073999</td>\n",
       "      <td>0.977400</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n"
     ]
    }
   ],
   "source": [
    "for arch in architectures_to_test:\n",
    "    print(arch)\n",
    "    learn = train_model(arch, epochs=3)   \n",
    "    print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a2f3d0-4916-43f4-ac39-797a92dda971",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e8d76-9d99-4e82-b22d-d15c7ddc29b1",
   "metadata": {},
   "source": [
    "We can use the predictions of multiple models to generate an ensemble prediction. I.e., use [_model ensembling_](https://nbviewer.org/github/alu042/DAT158-2022/tree/main/notebooks/DAT158-2.5-Random_forests_and_ensembling.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdf90f-0ca3-440c-abb9-3e847cd69acb",
   "metadata": {},
   "source": [
    "Here are some shortlisted architectures based on the experiments above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b4813be-3405-4fb2-97c0-fd094a0f19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "architectures = [\n",
    "    \"convnext_tiny_in22k\",\n",
    "    \"convnext_small_in22k\",\n",
    "    \"swin_s3_tiny_224\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561d23f8-0811-42b3-948a-4c87ae74abae",
   "metadata": {},
   "source": [
    "We train and save models using these architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44d705be-2a19-472d-a350-d0333f1473e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_tiny_in22k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.158087</td>\n",
       "      <td>0.117142</td>\n",
       "      <td>0.962700</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.107149</td>\n",
       "      <td>0.073173</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>01:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.066031</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>0.979500</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033730</td>\n",
       "      <td>0.060736</td>\n",
       "      <td>0.980600</td>\n",
       "      <td>01:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "convnext_small_in22k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.119185</td>\n",
       "      <td>0.078847</td>\n",
       "      <td>0.975900</td>\n",
       "      <td>01:48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.064803</td>\n",
       "      <td>0.056526</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.040895</td>\n",
       "      <td>0.045139</td>\n",
       "      <td>0.985900</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.023894</td>\n",
       "      <td>0.042914</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>02:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "swin_s3_tiny_224\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.273670</td>\n",
       "      <td>0.159262</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>01:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.162904</td>\n",
       "      <td>0.108674</td>\n",
       "      <td>0.964600</td>\n",
       "      <td>01:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.125018</td>\n",
       "      <td>0.081625</td>\n",
       "      <td>0.973400</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084699</td>\n",
       "      <td>0.076875</td>\n",
       "      <td>0.976200</td>\n",
       "      <td>02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n"
     ]
    }
   ],
   "source": [
    "for arch in architectures:\n",
    "    print(arch)\n",
    "    train_model(arch, savefn=arch)\n",
    "    print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a4bed-88d8-4211-bb99-1028c3c1107d",
   "metadata": {},
   "source": [
    "Grab all the saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c170bc11-114a-4fa8-a7e2-0a7907576a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('/home/alex/.fastai/data/cifar10/convnext_small_in22k.pkl'), Path('/home/alex/.fastai/data/cifar10/swin_s3_tiny_224.pkl'), Path('/home/alex/.fastai/data/cifar10/convnext_tiny_in22k.pkl')]\n"
     ]
    }
   ],
   "source": [
    "if kaggle:\n",
    "    models = list(Path('/kaggle/working/').glob(\"*.pkl\"))\n",
    "else:\n",
    "    models = list(dls.path.glob(\"*.pkl\"))\n",
    "    \n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3264531-e6e4-4114-9fe5-fa17cf63ab51",
   "metadata": {},
   "source": [
    "Get the predictions from each model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06d15812-9c99-4f09-880c-1fba7c6a57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from model #1/3 \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from model #2/3 \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from model #3/3 \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_probs = []\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    learn = load_learner(model, cpu=False)\n",
    "    learn.dls = dls\n",
    "\n",
    "    print(f\"Getting predictions from model #{i+1}/{len(models)} \")\n",
    "    probs, preds = learn.get_preds(dl=learn.dls.valid)\n",
    "    all_probs.append(probs)\n",
    "    \n",
    "    del learn\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f1e24-d563-4d58-806f-faf78a70fcc4",
   "metadata": {},
   "source": [
    "Combine the predictions to an ensemble prediction (a \"soft-voting ensemble\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "908221a2-bd24-4fa2-a3fe-c82257c8b53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10000, 10])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_probs = torch.stack(all_probs, dim=0)\n",
    "all_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63d86f0e-6d0c-4ea3-b7fa-2603708cc4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 10])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = all_probs.mean(axis=0)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "038129f0-0487-4e6c-9e53-1a2df9dc7468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.uint8(mean.argmax(dim=1))\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc976ec-deab-40eb-bf61-371e4c8b0e73",
   "metadata": {},
   "source": [
    "What is the accuracy of the ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "086af9f3-ad71-4b5f-b96e-d2de98703eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [int(l) for img, l in dls.valid.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce8f5e5b-1708-44c2-875b-1eaa0e1d5552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == labels).sum() / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db74b3c-c921-47fe-a2cc-e42fbc0d02ad",
   "metadata": {},
   "source": [
    "> Better than each single model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243f5fa-6330-444f-b88f-4f81a220322b",
   "metadata": {},
   "source": [
    "**How many images did we misclassify now?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec28867-fef8-4463-b3b4-1c8e759df009",
   "metadata": {},
   "source": [
    "(1-0.9878) * 10.000 = 122"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d182b-65c4-406f-a498-7fae000dac2c",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76a942-ed1d-45fd-80eb-28a26ef19038",
   "metadata": {},
   "source": [
    "> You know many more tricks from fastai. Use them to try to beat the above score. For example, would test-time augmentation (TTA) improve the results? What about additional data augmentation techniques? Perhaps you can find a better set of models for the ensemble?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
