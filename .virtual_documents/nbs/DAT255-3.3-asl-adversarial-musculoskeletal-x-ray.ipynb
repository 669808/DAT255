











from fastai.vision.all import *


# This is a quick check of whether the notebook is currently running on Kaggle, 
import os
kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', False)


kaggle


if kaggle:
    DATA = Path('/kaggle/input/mura-v11')
    DATA.ls()
else: 
    NB_DIR = Path.cwd()
    DATA = Path.home()/'data'/'mura-v11'
    DATA.mkdir(exist_ok=True)





if not kaggle: 
    # Download the data set using the Kaggle API 
    # (OBS: you need to provide your own credentials. See https://www.kaggle.com/docs/api)

    if not (DATA/'mura-v11.zip').exists():
        !kaggle datasets download -d cjinny/mura-v11 -p {DATA}
    else:
        print('File already downloaded')
    
    # Unzip the data
    
    # Check if already unzipped
        
    if not (DATA/'MURA-v1.1').exists():
        import shutil
        shutil.unpack_archive(DATA/'mura-v11.zip', DATA)
    else:
        print('Files already unpacked')


# We combine the train and valid dataframes into one, 
# and add a column to indicate whether the image is part of the validation set or not.

train_df = pd.read_csv(DATA/'MURA-v1.1'/'train_image_paths.csv', header=None)
valid_df = pd.read_csv(DATA/'MURA-v1.1'/'valid_image_paths.csv', header=None)

train_df['is_valid'] = False
valid_df['is_valid'] = True

df = pd.concat([train_df, valid_df], axis=0)

df.columns = ["study", "is_valid"]


df.head()





def get_label(fn):
    return fn.split("/")[-2].split("_")[-1]

def get_subject_id(fn):
    return fn.split("/")[-3][7:]

def get_bodypart(fn):
    return fn.split("/")[2]





df['label'] = df["study"].apply(get_label)


df['subject_id'] = df["study"].apply(get_subject_id)


df['bodypart'] = df["study"].apply(get_bodypart)


df.head()


df.label.value_counts()


len(df.subject_id.unique())











df.bodypart.value_counts()


bodypart = 'XR_WRIST'


df = df.loc[df.bodypart == bodypart]


df.is_valid.value_counts()


df.label.value_counts()








sz=224
item_tfms = Resize(sz)
batch_tfms = None
bs=64


dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), 
                   get_x=ColReader('study', pref=DATA), 
                   get_y=ColReader('label'),
                   splitter=ColSplitter(col='is_valid'), 
                   item_tfms=item_tfms, 
                   batch_tfms=batch_tfms)



dls = dblock.dataloaders(df, path=DATA, bs=bs)


dls.show_batch()








import timm


#timm.list_models("*convnext*")


model = 'convnext_tiny_in22ft1k'


if kaggle:
    model_dir = Path("/kaggle/working/models")
else: 
    model_dir = DATA/'models'


learn = vision_learner(dls, model, metrics=accuracy, model_dir=model_dir)


lr = learn.lr_find(suggest_funcs=(valley, steep, slide))


base_lr = (lr.valley + lr.slide)/2
print(base_lr)


# You can train for fewer epoch if you want to speed up the training
learn.fine_tune(3, base_lr)





interp = ClassificationInterpretation.from_learner(learn)


interp.plot_confusion_matrix()


interp.plot_top_losses(6, figsize=(15,10))


interp.print_classification_report()





learn.export(model_dir/f'mura_model-{model}.pkl')








learn = load_learner(model_dir/f'mura_model-{model}.pkl')


learn.training


df.loc[df.is_valid==True].head()


import torchvision.transforms as T


def fgsm_attack(image, epsilon, image_grad):
    """
    Perform the Fast Gradient Sign Method (FGSM) attack on an image.

    Parameters:
    image: The original image.
    epsilon: The epsilon value used to perturb the image.
    image_grad: The gradients of the image.

    Returns:
    perturbed_image: The perturbed image.
    """
    
    # Collect the element-wise sign of the image gradient
    sign_image_grad = image_grad.sign()
    # Create the perturbed image by adjusting each pixel of the input image
    perturbed_image = image + epsilon*sign_image_grad
    # Adding clipping to maintain [0,1] range
    perturbed_image = torch.clamp(perturbed_image, 0, 1)
    # Return the perturbed image
    return perturbed_image


def perturb(learn, img_path, epsilon):
    """
    Perform an adversarial attack on an image and return the perturbed image and the predicted labels.

    Parameters:
    learn: The Learner object.
    img_path: The path to the image to be perturbed.
    epsilon: The epsilon value used to perturb the image.

    Returns:
    perturbed_image: The perturbed image.
    predicted_label: The label predicted for the original image.
    perturbed_label: The label predicted for the perturbed image.
    """

    # Perform the transformations and instruct PyTorch to track the gradients
    img_t = T.ToTensor()(PILImage.create(img_path).resize((sz,sz)))
    img_t = img_t.requires_grad_(True)

    output = learn.model(img_t[None])

    # Here's the prediction before the attack
    predicted_label = torch.argmax(output)

    # Perform the attack
    loss = learn.loss_func(output, predicted_label)
    learn.zero_grad()
    loss.backward()
    img_t_grad = img_t.grad.data
    perturbed_image = fgsm_attack(img_t, epsilon, img_t_grad)

    # Here's the prediction after the attack
    perturbed_output = learn.model(perturbed_image[None])
    perturbed_label = torch.argmax(perturbed_output)
    # The difference between input and output
    diff_image = img_t - perturbed_image

    # We need to convert the image back to numpy for plotting
    perturbed_image = perturbed_image.detach().cpu().permute(1, 2, 0).numpy()
    diff_image = diff_image.detach().cpu().permute(1, 2, 0).numpy()*255
    
    return perturbed_image, learn.dls.vocab[predicted_label], learn.dls.vocab[perturbed_label], diff_image



def show_perturbed_img(original_image, predicted_label, perturbed_image, perturbed_label, diff_image, epsilon):
    """
    Display an original image and its perturbed version side by side.

    Parameters:
    original_image: The original image.
    predicted_label: The label predicted for the original image.
    perturbed_image: The perturbed image.
    perturbed_label: The label predicted for the perturbed image.
    epsilon: The epsilon value used to perturb the image.
    """
    
    f, ax = plt.subplots(1,3, figsize=(14,10))

    ax[0].imshow(PILImage.create(original_image).resize((sz,sz)))
    ax[0].axis('off')
    ax[0].set_title(f"Original image. Label: {predicted_label}")

    ax[1].imshow(perturbed_image)
    ax[1].axis('off')
    ax[1].set_title(f"Perturbed image. Label: {perturbed_label}; Epsilon: {epsilon}")

    ax[2].imshow(diff_image*255)
    ax[2].axis('off')
    ax[2].set_title(f"Difference (scaled)")
    

    plt.show()


epsilon=0.002


for test_img in random.choices(df.loc[df.is_valid==True].study, k=5):
    test_img = DATA/test_img
    perturbed_image, predicted_label, perturbed_label, diff_image = perturb(learn, test_img, epsilon)
    show_perturbed_img(test_img, predicted_label, perturbed_image, perturbed_label, diff_image, epsilon=epsilon)



