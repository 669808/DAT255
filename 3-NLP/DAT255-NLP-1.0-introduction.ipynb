{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb3be86",
   "metadata": {},
   "source": [
    "A.S. Lundervold, v.280223"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2b9ea",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05754006",
   "metadata": {},
   "source": [
    "This is a quick example of some techniques and ideas from natural language processing (NLP) and some approaches to NLP based on deep learning. The goal is to introduce some of the things going on in this field and for you better to understand some recent ideas and developments in deep learning.\n",
    "\n",
    "This and the following two notebooks serve as companions to the fastai-based material covered in Module 4 of the course (Lesson 5 and Chapter 9 of the textbook). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46c391",
   "metadata": {},
   "source": [
    "> NLP is an exciting area these days. Breakthroughs in deep learning for language processing recently initiated a revolution in NLP, and we're still in it. The best place to start exploring this is perhaps the HuggingFace community and library (at least if you want to get started right away playing around with using state-of-the-art NLP models): https://huggingface.co/. <br> <br><a href=\"https://huggingface.co/\"><img width=20% src=\"https://luxcapital-website-media.s3.amazonaws.com/wp-content/uploads/2019/12/23115642/Logo-600x554.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3bd56",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39baafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21518cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "  %pip install datasets\n",
    "  %pip install transformers\n",
    "  %pip install evaluate\n",
    "  %pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea325ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17260248",
   "metadata": {},
   "source": [
    "We'll use the excellent HuggingFace Transformers library, which covers all our natural language processing needs:\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/b253a30b83a0724f3f74f3f58236fb49ced8d7b27cb15835c9978b54e444ab08/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7265736f6c76652f6d61696e2f7472616e73666f726d6572735f6c6f676f5f6e616d652e706e67\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad307da5",
   "metadata": {},
   "source": [
    "We will not cover the library in any detail. If you're interested, take a look at the [HuggingFace course](https://huggingface.co/course/chapter1/1) and its documentation over at https://huggingface.co/transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c0448",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fcf954",
   "metadata": {},
   "source": [
    "We'll use the [IMDB dataset](https://huggingface.co/datasets/imdb) containing 50.000 movie reviews from IMDB, each labeled as either negative (0) or positive (1). It is split into 25.000 reviews for training and 25.000 reviews for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f23cb",
   "metadata": {},
   "source": [
    "The dataset is available via HuggingFace `datasets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f8a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3df4a28e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c52173f3c1e4433abbeb22371cc17af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90aae66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3719",
   "metadata": {},
   "source": [
    "## Make a sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99509d",
   "metadata": {},
   "source": [
    "As the training process takes a long time, we create a small sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2e3259",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ee11e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-db2211c4cbcbe9c5.arrow and /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-728f846228169c23.arrow\n"
     ]
    }
   ],
   "source": [
    "if sample:\n",
    "    dataset = dataset['train']\n",
    "    dataset = dataset.train_test_split(train_size=0.2, shuffle=True, seed=42)['train']\n",
    "    dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05875eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e357cc5",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4942af",
   "metadata": {},
   "source": [
    "The training data is stored under `train`, the test data under `test`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c430f0",
   "metadata": {},
   "source": [
    "Here are two training instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753a84a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"this is a great movie. I love the series on tv and so I loved the movie. One of the best things in the movie is that Helga finally admits her deepest darkest secret to Arnold!!! that was great. i loved it it was pretty funny too. It's a great movie! Doy!!!\",\n",
       "  'I\\'m sorry to say that there isn\\'t really any way, in my opinion, that an Enzo would really be able to keep up with a Saleen S7 Twin Turbo. The power to weight advantage possessed by the S7 would just be too great. The S7 has a power:weight ratio of 3.93 lbs/hp while the Enzo has 4.61 lbs/hp. The S7s low end is much better too. Sorry Ferrari fans but the Saleen just gets it done so much better.<br /><br />As for other parts of this film, I just have to say it\\'s so substandard as to be pathetic. The story is way too weak. The acting in this lemon is worse than daytime soaps.<br /><br />I can say that as far as it being a treatise on negative psychology its kind of a gem. This film is nothing if not a glaring definition as to what narcissism and sociopathy are all about. Its all about these rich punks getting their rocks off while showing only traces of feigned remorse for all the innocent road users they cause injury or death too.<br /><br />I can\\'t give the film a \"1 Star\" rating because it didn\\'t compel me to actually walk out of the theater. I also think that having an amazingly beautiful brunette with killer blue eyes as the leading female saves it from being completely abysmal....although there is no way her singing would put her on the cover of \\'Variety\\'.<br /><br />ps: the guy who plays Jason is SOOOOO the skid row version of James Vanderbeek.'],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c86615d",
   "metadata": {},
   "source": [
    "We can print them a in a more readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6679616f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is a great movie. I love the series on tv and so I loved the movie. One of the best things in the movie is that Helga finally admits her deepest darkest secret to Arnold!!! that was great. i loved it it was pretty funny too. It's a great movie! Doy!!!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ee9562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a25a69",
   "metadata": {},
   "source": [
    "> **How do we represent the text for consumption by a machine learning model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d1df1",
   "metadata": {},
   "source": [
    "> **How can a computer read??**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4793ff9",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/7d5ed540c87d660cae46ca0d2055d760f786bea36513bb1a0b0784d47cef45b1/687474703a2f2f322e62702e626c6f6773706f742e636f6d2f5f2d2d75564865746b5549512f54446165356a476e6138492f4141414141414141414b302f734253704c7564576d63772f73313630302f72656164696e672e676966\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3de3a",
   "metadata": {},
   "source": [
    "# Prepare the data: tokenization and numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24507e5f",
   "metadata": {},
   "source": [
    "For a computer, everything is numbers. We have to convert the text to a series of numbers and then feed those to the computer.\n",
    "\n",
    "This can be done in two widely used steps in natural language processing: **tokenization** and **numericalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106b8e4",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b558902",
   "metadata": {},
   "source": [
    "In tokenization, the text is split into single components or units called tokens. In the context of deep learning, tokenization aims to convert a sequence of characters into a sequence of tokens in a way that enables accurate and efficient processing by deep learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4844a10",
   "metadata": {},
   "source": [
    "Multiple tokenization strategies–word, character and subword-based–can tackle these and other issues. Examples include **rule-based splitting of sentences** (used by ULMFiT and Transformer XL and others), **WordPiece** (used by BERT and others), **SentencePiece** (used by XLM and others), and **Byte-Pair encoding** (used by GPT models (including ChatGPT) and others).\n",
    "\n",
    "Let's take a look at some of the ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33921dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"Here's a sentence to be tokenized by a tokenizer, and it includes the non-existent word graffalacticus\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6732a",
   "metadata": {},
   "source": [
    "### Character tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8858b823",
   "metadata": {},
   "source": [
    "Perhaps the simplest tokenization strategy is to split the text into characters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8829d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'r', 'e', \"'\", 's', ' ', 'a', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 't', 'o', ' ', 'b', 'e', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'd', ' ', 'b', 'y', ' ', 'a', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r', ',', ' ', 'a', 'n', 'd', ' ', 'i', 't', ' ', 'i', 'n', 'c', 'l', 'u', 'd', 'e', 's', ' ', 't', 'h', 'e', ' ', 'n', 'o', 'n', '-', 'e', 'x', 'i', 's', 't', 'e', 'n', 't', ' ', 'w', 'o', 'r', 'd', ' ', 'g', 'r', 'a', 'f', 'f', 'a', 'l', 'a', 'c', 't', 'i', 'c', 'u', 's']\n"
     ]
    }
   ],
   "source": [
    "characters = [c.lower() for c in example_sentence]\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb929a7f",
   "metadata": {},
   "source": [
    "Unicode and ASCII are well-known examples of character encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100b4edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Here's a sentence to be tokenized by a tokenizer, and it includes the non-existent word graffalacticus\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence.encode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7d6aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 114, 101, 39, 115, 32, 97, 32, 115, 101, 110, 116, 101, 110, 99, 101, 32, 116, 111, 32, 98, 101, 32, 116, 111, 107, 101, 110, 105, 122, 101, 100, 32, 98, 121, 32, 97, 32, 116, 111, 107, 101, 110, 105, 122, 101, 114, 44, 32, 97, 110, 100, 32, 105, 116, 32, 105, 110, 99, 108, 117, 100, 101, 115, 32, 116, 104, 101, 32, 110, 111, 110, 45, 101, 120, 105, 115, 116, 101, 110, 116, 32, 119, 111, 114, 100, 32, 103, 114, 97, 102, 102, 97, 108, 97, 99, 116, 105, 99, 117, 115]\n"
     ]
    }
   ],
   "source": [
    "# Unicode values for each character in the example sentence\n",
    "print([ord(c) for c in example_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87863eab",
   "metadata": {},
   "source": [
    "This is typically not a useful strategy for deep learning, as it is too granular. It is, however, useful for some applications, such as spelling correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111977b",
   "metadata": {},
   "source": [
    "### Splitting text into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10b9c0",
   "metadata": {},
   "source": [
    "Here's a way to tokenize: simply split into words by spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83c42a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = example_sentence.split(\" \")\n",
    "tokens = {v: k for k, v in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31820ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Here's\": 0,\n",
       " 'a': 7,\n",
       " 'sentence': 2,\n",
       " 'to': 3,\n",
       " 'be': 4,\n",
       " 'tokenized': 5,\n",
       " 'by': 6,\n",
       " 'tokenizer,': 8,\n",
       " 'and': 9,\n",
       " 'it': 10,\n",
       " 'includes': 11,\n",
       " 'the': 12,\n",
       " 'non-existent': 13,\n",
       " 'word': 14,\n",
       " 'graffalacticus': 15}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e01e65",
   "metadata": {},
   "source": [
    "One would never use this in practice, as it's very inefficient and uses no features of language except that words tend to, in some languages, be separated by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f916c60",
   "metadata": {},
   "source": [
    "Among other things, we lose punctuation and the fact that some words are contractions of multiple words (for example \"here's\", \"isn't\", and \"don't\"). By specifying a set of rules, we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc8e08",
   "metadata": {},
   "source": [
    "<img src=\"https://spacy.io/images/tokenization.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9bdf9",
   "metadata": {},
   "source": [
    "### Rule-based splitting of sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7ef09",
   "metadata": {},
   "source": [
    "Here's a better approach, using the NLP library `spaCy`. We install spaCy and download a set of rules for tokenizing English text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d273704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import spacy\n",
    "except:\n",
    "    %pip install spacy\n",
    "    import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1787d547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy model loaded\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Spacy model loaded\")\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abd65f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      "a\n",
      "sentence\n",
      "to\n",
      "be\n",
      "tokenized\n",
      "by\n",
      "a\n",
      "tokenizer\n",
      ",\n",
      "and\n",
      "it\n",
      "includes\n",
      "the\n",
      "non\n",
      "-\n",
      "existent\n",
      "word\n",
      "graffalacticus\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_sentence)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d40cdf",
   "metadata": {},
   "source": [
    "### Subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99980c72",
   "metadata": {},
   "source": [
    "With word-based tokenization we typically need a very large vocabulary to encode all possible words. Subword tokenization is a mid-way between character encoding and full word encoding and is based on splitting words into word pieces. Common words can be assigned their own token while rare words can be split into pieces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265fa1d",
   "metadata": {},
   "source": [
    "Modern subword tokenizers tend to be _trained_ on the text corpus you're interested in (or pre-trained on a large corpus that you want to train a model that you can use for fine-tuning). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d2c99",
   "metadata": {},
   "source": [
    "Here's an example of a subword tokenizer: the token splitting algorithm of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6ee4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efa5d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "999f7bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'be',\n",
       " 'token',\n",
       " '##ized',\n",
       " 'by',\n",
       " 'a',\n",
       " 'token',\n",
       " '##izer',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'non',\n",
       " '-',\n",
       " 'existent',\n",
       " 'word',\n",
       " 'graf',\n",
       " '##fa',\n",
       " '##la',\n",
       " '##ctic',\n",
       " '##us']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(example_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785dd7d",
   "metadata": {},
   "source": [
    "### Byte-Pair encoding: an example of training an encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17713e",
   "metadata": {},
   "source": [
    "When faced with a particular text corpus the above rules-based tokenizers can often be both wasteful (with superfluous tokens for words that doen't appear in your corpus) and inefficient (for example, lacking tokens that can represent important and often-used words in your particular corpus). \n",
    "\n",
    "Tokenizers based on _training_, i.e. identification of important words or word pieces, can therefore be useful, and this is thus part of most modern tokenizers. An example is the **byte pair encoding** used by, among others, GPT models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa00578",
   "metadata": {},
   "source": [
    "The Byte Pair Encoding (BPE) algorithm was introduced by Philip Gage in 1994 for data compression (_\"a simple general-purpose data compression algorithm\"_), based on identifying common byte pairs. Here's a copy of the original article http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM. See also Wikipedia for a simple example of BPE used for data compression: https://en.wikipedia.org/wiki/Byte_pair_encoding\n",
    "\n",
    "The procedure is roughly the following: \n",
    "\n",
    "```\n",
    "1. Add identifiers marking the end of each word\n",
    "2. Calculate the word frequencies in the text corpus\n",
    "3. Split the words into characters and calculate the character frequencies\n",
    "4. From character tokens, count the frequency of consecutive byte pairs and merge the most frequent byte pair\n",
    "5. Continue until a manually defined iteration limit is reached, or the token limit is reached. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520022c3",
   "metadata": {},
   "source": [
    "> This is a greedy algorithm. Non-greedy variants exist and other tweaks to BPE are in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ab9231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65d3feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96915623",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5efa4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d494501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(dataset['train']['text'],trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1e5bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence_bpe = tokenizer.encode(example_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04bd7e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=27, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2447c2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'be',\n",
       " 'token',\n",
       " 'ized',\n",
       " 'by',\n",
       " 'a',\n",
       " 'token',\n",
       " 'izer',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'includes',\n",
       " 'the',\n",
       " 'non',\n",
       " '-',\n",
       " 'existent',\n",
       " 'word',\n",
       " 'gr',\n",
       " 'aff',\n",
       " 'al',\n",
       " 'actic',\n",
       " 'us']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f90155e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2160, 7, 83, 65, 9932, 151, 174, 13161, 1445, 273, 65, 13161, 14468, 12, 157]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a68b6f",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e7464",
   "metadata": {},
   "source": [
    "We convert tokens to numbers by making a list of all the tokens that have been used and assign them to numbers. This has already been taken care of for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b3f107a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2160, 7, 83, 65, 9932, 151, 174, 13161, 1445, 273, 65, 13161, 14468, 12, 157]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence_bpe.ids[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5d1b9",
   "metadata": {},
   "source": [
    "# Fine-tuning pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511a44a",
   "metadata": {},
   "source": [
    "The advent of the **Transformers models** has revolutionized the field of natural language processing. Therefore, when faced with any NLP task for which deep learning is applicable, everyone tends to turn to Transformers models. Furthermore, one typically uses _pre-trained models_. In other words, models that have already been trained on large-scale NLP tasks and thus contain representations that typically provide useful starting points for new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559300e5",
   "metadata": {},
   "source": [
    "## Text representation for pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4b8e9",
   "metadata": {},
   "source": [
    "When using pre-trained models, we must pre-process the text exactly as expected by the model. In other words, that we use the expected tokenization, numericalization, padding, and truncation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2858fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6c5d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbee4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "212463ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9eac1be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab40400",
   "metadata": {},
   "source": [
    "## Fine-tune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3ee57",
   "metadata": {},
   "source": [
    "We'll fine-tune a BERT model on our IMDB dataset. (Note that this is where it's best to use a sample of the dataset. Otherwise the training process will take a long time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2c882c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c8e13",
   "metadata": {},
   "source": [
    "**Define the model and its preprocessing steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eae28f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ae41537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae5148",
   "metadata": {},
   "source": [
    "**Set up our evaluation metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47a58029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b2e7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7eab2",
   "metadata": {},
   "source": [
    "**Configure the training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "851fe61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee95c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase this to improve performance (at the cost of computational time), \n",
    "# especially if you're training on the full data set.\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4cc6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\".\", num_train_epochs=num_train_epochs, \n",
    "                                  evaluation_strategy=\"epoch\", report_to='all',\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d77a7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7733603",
   "metadata": {},
   "source": [
    "**Train and evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c02b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if kaggle:\n",
    "    import os\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5947fb02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alex/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.398279</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "Model weights saved in ./checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.3765945129394531, metrics={'train_runtime': 112.3994, 'train_samples_per_second': 35.587, 'train_steps_per_second': 4.448, 'total_flos': 1052444221440000.0, 'train_loss': 0.3765945129394531, 'epoch': 1.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d674f",
   "metadata": {},
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "040dd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\"This movie was not pretty good.\", \"You should miss it!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a24186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = tokenizer(test_data, return_tensors=\"pt\", padding=True)[\"input_ids\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d6d43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d92b1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "outputs.logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c60d85",
   "metadata": {},
   "source": [
    "### Create an app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff5797de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def classify_review(review):\n",
    "        tokenized_text = tokenizer(review, return_tensors=\"pt\", padding=True)[\"input_ids\"].cuda()\n",
    "        response = model(tokenized_text)\n",
    "        sentiment = int(response.logits.argmax(-1))\n",
    "        if sentiment:\n",
    "            return \"Positive\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "\n",
    "\n",
    "textbox = gr.Textbox()\n",
    "\n",
    "demo = gr.Interface(classify_review, inputs=\"text\", outputs=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9782948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1488e",
   "metadata": {},
   "source": [
    "# Embeddings and using pre-trained text encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078799a1",
   "metadata": {},
   "source": [
    "## Some key concepts that were mentioned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa683f9",
   "metadata": {},
   "source": [
    "In the lecture, I told a short story about the following key concepts, widely used in modern deep learning:\n",
    "\n",
    "* Embeddings and representations\n",
    "* Word2Vec\n",
    "* Language Models\n",
    "* Training language models\n",
    "* Reusing representations for other tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a284f9",
   "metadata": {},
   "source": [
    "## TensorFlow Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d563a",
   "metadata": {},
   "source": [
    "These concepts were introduced in the lecture using the TensorFlow Embedding Projector: http://projector.tensorflow.org/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d149382c",
   "metadata": {},
   "source": [
    "<a href=\"http://projector.tensorflow.org/\"><img src=\"https://raw.githubusercontent.com/HVL-ML/DAT255/main/3-NLP/assets/TensorFlowProjector.png\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f14ffdfc",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/HVL-ML/DAT255/raw/main/3-NLP/assets/TensorFlowProjector.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c9628",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a52ebc21087bab326821c7f3b92bdc148f3efe050370d2c4599ad1d311b24304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
