{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691d5ef9-f9b2-4435-b753-6e49628cd480",
   "metadata": {},
   "source": [
    "ASL, v.0602323"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579e4dd-b2a3-4928-b913-552a972ccbd4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550462f0-c14f-41bd-8e0c-05178529382a",
   "metadata": {},
   "source": [
    "In this notebook, we use Transformers on the same task as in the previous notebook (`DAT255-NLP-2.0-MedTweets-fastai-ULMFiT.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b865c9-4f82-4aae-b596-a7c9ed6e9d0e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265cca-ad9e-4099-9540-10cbbf5f852d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4359823-c8e1-42be-8bd8-f1c1ccd477c5",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe9203-2af9-4ab1-adf4-7f30fade157b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cb89e7-e7c8-4be7-9f5f-0a6a1777db72",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc4e5d-b45a-4be0-a4f0-fff3d4c4f269",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    DATA = Path(\"/content/gdrive/MyDrive/Colab Notebooks/dat255-data\")\n",
    "    DATA.mkdir(exist_ok=True)\n",
    "if not colab:\n",
    "    DATA=Path('./data')\n",
    "    DATA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0df86d-2e01-4ee4-a07e-deaeaa1fd197",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05267cd-0918-419c-9dd8-1b22c5c1ed3c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3387f-6096-42f7-af84-0e278e9515ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce49679",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8112181-8b08-45f9-b039-9c8825ca2781",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Verify that the transformers library is installed and operational\n",
    "print(transformers.pipeline('sentiment-analysis')('this is great!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f4d78-51c4-4616-b413-d16eaa3a2a01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          PreTrainedModel, BertModel, BertForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, GPT2ForSequenceClassification, \n",
    "                          RobertaForSequenceClassification)\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33283de-aff5-4677-8977-04503956e199",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MedWeb using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5bd54a-91ed-4905-bf08-d9191e660be2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Load the data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dd6ab-57ec-47fb-96d3-2f8ea31590e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/HVL-ML/DAT255/raw/main/3-NLP/data/medwebdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59355ecf-9ea6-43f2-b0a6-d30d69e4d7ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For convenience, we combine all the labels into one vector stored under `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a28a66-35a5-40e7-8a54-037015239657",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(['is_test','labels'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe100807-13e2-4f39-81a4-c803b9538a00",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['labels'] = df.apply(lambda x: [x[c] for c in df.columns[2:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb83b17-8d4f-4938-b751-329a5285e532",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f064f2-05ab-49fd-bffe-c38557a85bf2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Set up the transformers model. There are multiple possible models to try (at the time of writing, HuggingFace has 146,394 models in its library). \n",
    "\n",
    "One interesting option is the [PubMedBERT model](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract) created by Microsoft Research by training a BERT model on 14 million abstracts of PubMed articles. Have a look at the blog post [Domain-specific language model pretraining for biomedical natural language processing](https://www.microsoft.com/en-us/research/blog/domain-specific-language-model-pretraining-for-biomedical-natural-language-processing/) and the accompanying paper. \n",
    "\n",
    "A related model is the BioMed-RoBERTa model from AllenAI: https://huggingface.co/allenai/biomed_roberta_base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64796a7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The more recent GPT models are also interesting options (for example the BioMedLM model from Stanford CRFM: https://huggingface.c/stanford-crfm/BioMedLM). Unfortunately, GPT models require enormous amounts of computing resources compared to many alternatives. See the end of the notebook for an example run of the GPT-2 model BioMedLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa595b-af89-414f-a393-bd4947c1946f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model_name = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
    "#model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "model_name = \"allenai/biomed_roberta_base\"\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf01dda1-4107-452f-97bf-e48d420d90cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to tokenize the data in the same way as was done for the original dataset and create a data set compatible with HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2769602-4e6d-411c-b55b-89eb70134b50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6b03c-d5e7-405b-a6bf-cf8e293d0441",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df, split='train').train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113adc5-ec52-4b88-9d63-1ff9bb974725",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2aeb9-4923-40bf-82cb-2c1e97c758b0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edf869-145a-4d80-9c49-dc32e056e343",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_encode(examples):\n",
    "    return tokenizer(examples[\"Tweet\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497d345-b66d-4f66-a9e1-d9bd6c04f968",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ds['train'].column_names\n",
    "cols.remove('labels')\n",
    "ds_enc = ds.map(tokenize_and_encode, batched=True, remove_columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e36eec-4ba5-426d-b644-41ae6bdd0d1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ds_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c56184-15e0-4593-b811-5d7256222682",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc33f4b-19b6-4b1a-b6ce-38932fc79a1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_labels=8\n",
    "\n",
    "if \"roberta\" not in model_name:\n",
    "    print(\"Assuming a BERT model\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, \n",
    "                                                        problem_type=\"multi_label_classification\", \n",
    "                                                        num_labels=num_labels)\n",
    "\n",
    "elif \"roberta\" in model_name:\n",
    "    print(\"Assuming a RoBERTa model\")\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name, \n",
    "                                                        problem_type=\"multi_label_classification\", \n",
    "                                                        num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bf238-5e42-4380-abe0-dc694017155f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We define some metrics to use when scoring on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c19021-aab7-4fc7-b7b0-945b1ebfc219",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True): \n",
    "    y_pred = torch.from_numpy(y_pred)\n",
    "    y_true = torch.from_numpy(y_true)\n",
    "    if sigmoid: \n",
    "        y_pred = y_pred.sigmoid()\n",
    "    return ((y_pred>thresh)==y_true.bool()).float().mean().item()\n",
    "\n",
    "def f1score_thresh(y_pred, y_true, average='micro',thresh=0.5, sigmoid=True): \n",
    "    y_pred = torch.from_numpy(y_pred)\n",
    "    y_true = torch.from_numpy(y_true)\n",
    "    if sigmoid: \n",
    "        y_pred = y_pred.sigmoid()\n",
    "    return f1_score(y_true, y_pred>thresh, average='micro')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {'accuracy_thresh': accuracy_thresh(predictions, labels),\n",
    "           'f1score_micro_thresh': f1score_thresh(predictions, labels, average='micro'),\n",
    "           'f1score_macro_thresh': f1score_thresh(predictions, labels, average='macro')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578303e-dfdf-457f-91a9-dc38741313b9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "..and then the training setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88e2a3-34f8-4a26-819f-091eade0e352",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_train_epochs = 6\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=50,\n",
    "    weight_decay=0.01, \n",
    "    save_steps=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04674d2-5d44-461f-890c-ebf58d2e1f4a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have to modify the loss function to deal with multilabel problems. Here's a way to do it (from https://discuss.huggingface.co/t/fine-tune-for-multiclass-or-multilabel-multiclass/4035/9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72343b0-ccbe-4f9c-a8eb-0a49e613b429",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MultilabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                        labels.float().view(-1, self.model.config.num_labels))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1039ee-abe0-42d9-bf9a-fa440f1859c1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer = MultilabelTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds_enc[\"train\"],\n",
    "    eval_dataset=ds_enc[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e46042-53d8-4c77-87b2-c22df00cc963",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how the model does without any training on the MedWeb data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ceeb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa62db2-5379-40ad-b9d7-c4a22abbf46f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then fine-tune it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f8a8f-4987-44bc-826f-7ad4412c76cb",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2546bb5",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57898f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03daf6a6-36c0-483c-916e-945f377c1644",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "tags": []
   },
   "source": [
    "## How does it compare to other approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719a508-8072-4871-b95f-0a1a885ccc94",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From the [original article](https://www.jmir.org/2019/2/e12783/) from 2019 that presented the data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c211c3-bb8c-4bb5-978e-10f2eabc78a5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://github.com/MMIV-ML/ELMED219-2022/raw/main/Lab2-NLP/assets/medweb_results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de2dd2-06c3-49c8-9f63-bf8e4e154122",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The \"NAIST-en\" models are _\"ensembles of hierarchical attention network and deep character-level convolutional neural network with loss functions (negative loss function, hinge, and hinge squared)\"_. I.e. also deep learning-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0e9e4b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Extra: using a GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f722980",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at stanford-crfm/BioMedLM were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at stanford-crfm/BioMedLM and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"stanford-crfm/BioMedLM\" # This is a GPT2 model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d3c37b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# GPT2\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ab300",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, \n",
    "                                                        problem_type=\"multi_label_classification\", \n",
    "                                                        num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52447361",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a9212",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_train_epochs = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f3bbed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=50,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2c068",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trainer = MultilabelTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds_enc[\"train\"],\n",
    "    eval_dataset=ds_enc[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6a9f0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "42GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eb8c01f",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3840\n",
      "  Number of trainable parameters = 2594268160\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3840' max='3840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3840/3840 33:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Thresh</th>\n",
       "      <th>F1score Micro Thresh</th>\n",
       "      <th>F1score Macro Thresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.139600</td>\n",
       "      <td>0.256822</td>\n",
       "      <td>0.951563</td>\n",
       "      <td>0.797054</td>\n",
       "      <td>0.797054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>0.163555</td>\n",
       "      <td>0.966211</td>\n",
       "      <td>0.865579</td>\n",
       "      <td>0.865579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./checkpoint-500\n",
      "Configuration saved in ./checkpoint-500/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-500/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-1000\n",
      "Configuration saved in ./checkpoint-1000/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-1000/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-1500\n",
      "Configuration saved in ./checkpoint-1500/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-1500/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./checkpoint-2000\n",
      "Configuration saved in ./checkpoint-2000/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-2000/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-2500\n",
      "Configuration saved in ./checkpoint-2500/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-2500/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-3000\n",
      "Configuration saved in ./checkpoint-3000/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-3000/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./checkpoint-3500\n",
      "Configuration saved in ./checkpoint-3500/config.json\n",
      "The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at ./checkpoint-3500/pytorch_model.bin.index.json.\n",
      "tokenizer config file saved in ./checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./checkpoint-3500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 640\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3840, training_loss=0.2673093371093273, metrics={'train_runtime': 2015.1392, 'train_samples_per_second': 1.906, 'train_steps_per_second': 1.906, 'total_flos': 1032739377500160.0, 'train_loss': 0.2673093371093273, 'epoch': 2.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14314f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:huggingface]",
   "language": "python",
   "name": "conda-env-huggingface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
